<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prediction of material defects and phase formation using deep learning and transfer learning</title>
				<funder>
					<orgName type="full">Innovative Metal Processing (IMPACT)</orgName>
				</funder>
				<funder ref="#_CubrfdG">
					<orgName type="full">EPSRC CDT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09">September 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Leicester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prediction of material defects and phase formation using deep learning and transfer learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09">September 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">26771B1E11C1D1C6980FFD58CF470D7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-13T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning has been successfully employed in computer vision, speech processing, and natural language processing. However, when machine learning is applied to materials study, many challenges remain, and they include small datasets, tricky manual feature (descriptor) engineering, isolated models and poor interpretability. In this project, possible solutions to these challenges have been explored using deep learning and transfer learning.</p><p>For the challenge of small datasets, fully connected deep neural network and tree-based models were used to predict solidification cracking susceptibility of stainless steels with a dataset of 487 samples. It is found that deep neural network with pre-training and finetuning improves prediction accuracy, and tree-based models reveal the relative importance of input variables.</p><p>To overcome the challenge of tricky manual feature engineering in predicting phase formation in inorganic substances and compounds properties, I proposed a general and transferable deep learning framework as follows: (1) mapping raw data to pseudo images with periodic table structure, (2) automatically extracting features through convolutional neural networks, (3) transferring knowledge by sharing features extractors between models. The proposed deep learning models outperformed previous models in predicting glass-forming ability using a medium dataset of 16k samples and compounds properties using a big dataset of 228k samples. The developed transfer learning model for multi-principal element alloys can distinguish five phases (BCC, FCC, HCP, amorphous, mixture) with high scores (0.94) in a small dataset of 345 samples. The transfer learning model for phase prototypes can discriminate 170 phase prototypes with an accuracy of 0.9 in a dataset of 17k inorganic substances. Periodic table knowledge embedded in data representations and knowledge shared between models is beneficial for tasks with small datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table list</head> <ref type="table" target="#tab_3">5</ref><p>.10 Comparison of experimental results with the predictions by CNN1, CNN2, CNN3, SNN3 and SNN4 in RE6Fe72B22. The red indicates the wrong prediction, and the green indicates the right prediction. Superscript GT denotes this composition is in our dataset; superscript * denotes this composition is not in our dataset, but some other compositions of this alloy system are in our dataset.                          <ref type="table" target="#tab_0">4</ref>.3, processing parameters are the same: Th=3.18 mm, I=100 A, , U=12 V, Ve=4.23 mm/s, strain=3%. . 55 Figure <ref type="figure" target="#fig_21">4</ref>. 16 The prediction of 8 stainless steels' total crack length (mm) dependence on P and S, compositions used in predictions are shown in Table <ref type="table" target="#tab_0">4</ref>. 4, processing parameters are the same: Th=3.18 mm, I=100 A, U=12 V, Ve=4. 23  </p><note type="other">Figure list</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning is a powerful tool and an important complement to experiment, theory, and modeling. However, there are many challenges in machine learning for materials research: e.g., small datasets, tricky manual feature engineering, poor interpretability, poor transferability. These challenges demand some tailored approaches. How to fully exploit limited data, automatic feature engineering, existing models and domain expertise is the key to effectively applying machine learning in materials research. This thesis shows our attempts to tackle these challenges through deep learning and transfer learning.  2 Literature review on machine learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition of machine learning</head><p>Machine learning (ML), which is also known as statistical learning, data mining, and pattern recognition, is an important branch of artificial intelligence (AI) <ref type="bibr" target="#b79">1,</ref><ref type="bibr">2</ref> . It is a powerful technique that enables us to build adaptive mathematical models that automatically improve performance through example data or experience without explicit programming. Machine learning, especially its newest progress deep learning (DL), has achieved state-of-art performance in a variety of applications, e.g. search engine, spam email filtering, recommendation system, autonomous vehicles, computer vision (CV), speech recognition, natural language processing (NLP) <ref type="bibr">3,</ref><ref type="bibr">4</ref> . It is increasingly being exploited in science researches <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr" target="#b86">[8]</ref><ref type="bibr" target="#b87">[9]</ref><ref type="bibr" target="#b88">[10]</ref><ref type="bibr" target="#b89">[11]</ref><ref type="bibr" target="#b90">[12]</ref><ref type="bibr" target="#b91">[13]</ref> . The data-based and data-driven method has been recognized as the 4 th paradigm <ref type="bibr" target="#b92">14</ref> of science and a essential complement to experiment, theory, and simulation.    Feature transformation involves increasing/decreasing features dimensional e.g., principal component analysis (PCA) and linear discriminant analysis (LDA). Conventional machine learning is a combination of manual feature engineering and shallow leaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine learning workflow</head><p>Deep learning consists of automatic feature engineering and shallow learning. Thus, raw data, e.g., images, are used as direct input. Transfer learning was proposed to share knowledge between different tasks <ref type="bibr" target="#b93">15,</ref><ref type="bibr" target="#b94">16</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Basic conceptions of machine learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Data, classification &amp; regression</head><p>Data is the most important component (lifeblood) for a machine learning system. It can be structured data e.g., Excel datasheets, comma-separated values (CSV) data, and unstructured data e.g., image, text, sound, time serial data.</p><p>A piece of data is also known as one entry, or an example, or an instance, depending on different scenarios. A dataset is a finite set of examples and their corresponding labels (targets). If labels exist for all entries in a dataset, it is a supervised learning task; if no label is available, it is an unsupervised learning task; if only partial labels exist, it is called a semi-supervised learning process. Labels can be numerical-continuous or categorical.</p><p>In the first case, the process is called regression, while in the second, it is called classification. A dataset used in supervised learning can be represented as D = {X, Y}. In a dataset of structured data, input (features, which are also known as attributes and descriptors) can be represented as X = {ğ‘¥ 1 âƒ—âƒ—âƒ— , ğ‘¥ 2 âƒ—âƒ—âƒ—âƒ— , â€¦, ğ‘¥ ğ‘› âƒ—âƒ—âƒ—âƒ— } where feature vector ğ‘¥ ğ‘– âƒ—âƒ—âƒ— âˆˆ â„ ğ‘š .</p><p>Output (target) can be represented Y = {ğ‘¦ 1 , ğ‘¦ 2 , â€¦ , ğ‘¦ ğ‘› } where ğ‘¦ ğ‘– âˆˆ â„ 1 in single-task problems and ğ‘¦ ğ‘– âˆˆ â„ + in multi-task problems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Training &amp; testing datasets</head><p>In machine learning, a dataset is often split into training and testing datasets. The former is used for training a machine learning model and the latter is used for testing the performances of a trained model. If early-stopping technique is used, a small part of training dataset will be held out as validation dataset. In dataset division, training and testing datasets must reflect the original distribution of the dataset. In machine learning, we need to select the best fit model which is neither underfit nor overfit (see Figure <ref type="figure" target="#fig_12">2</ref>.3). The problem of underfitting will appear if we use an excessively simple model to approximate a complex dataset. For example, when the number of neurons is small in a neural network regression, underfitting occurs because the network lacks sufficient internal parameters (ability) to capture the complete characteristics of a dataset. Both training error and testing error are large in underfitting. It can be easily solved by increasing the complexity of model (e.g., increase the number of neurons in a network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Overfit &amp; underfit</head><p>In overfitting, the training error can be lowered to a very small value, but testing error turn out to be large. It indicates that the network only memorizes the training data, but it has not learned to generalize to new instances. A few techniques have been designed</p><p>to avoid overfitting and improve the generalization, e.g., early stopping, regularization, drop out, batch normalization <ref type="bibr">3,</ref><ref type="bibr" target="#b96">18</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Loss function, backpropagation algorithm &amp; regularization</head><p>The training process of many machine learning models e.g., neural network, is a process of minimizing the loss function though updating the internal parameters (e.g. weights and biases) of models. A typical loss function often used in machine learning is mean square error (MSE):</p><formula xml:id="formula_0">MSE = 1 ğ‘ âˆ‘(ğ‘’ ğ‘– ) 2 ğ‘ ğ‘–=1 = 1 ğ‘ âˆ‘(ğ‘¡ ğ‘– -ğ‘ ğ‘– ) 2 ğ‘ ğ‘–=1</formula><p>where ğ‘¡ ğ‘– is the target value and ğ‘ ğ‘– is the prediction value of the model based on the input data.</p><p>To improve generalization and reduce the risk of overfitting, a regularized loss function MSEreg, which combines mean square error with mean square weight (MSW), is also usually employed:</p><formula xml:id="formula_1">ğ‘€ğ‘†ğ¸ ğ‘Ÿğ‘’ğ‘” = Î³MSE + (1 -Î³)MSW MSW = 1 ğ‘› âˆ‘ ğ‘Š ğ‘— 2 ğ‘› ğ‘—=1</formula><p>where Î³ is the performance ratio (a hyper-parameter either given by researchers or automatically determined by optimization algorithm); ğ‘Š ğ‘— is the j-th weight.</p><p>Minimization of mean square weight entails small values of internal parameters, which assures the model responses smoothly and reduces the risk of overfitting.</p><p>Gradient descent algorithm <ref type="bibr" target="#b97">19</ref> (backpropagation algorithm) updates the parameters of a model in the direction in which the loss function decreases most rapidly, i.e. the opposite direction of gradient. In an iteration, parameters are updated as follows:</p><formula xml:id="formula_2">ğ‘¥ ğ‘˜+1 =ğ‘¥ ğ‘˜ -ğ›¼ ğ‘˜ ğ‘” ğ‘˜</formula><p>where ğ‘¥ ğ‘˜+1 is the updated parameters, ğ‘¥ ğ‘˜ is the old parameters, ğ‘” ğ‘˜ is the gradient, and ğ›¼ ğ‘˜ is the learning rate.</p><p>Automatic determination of the optimal regularization parameters is always desired.</p><p>Bayesian regularization proposed by MacKay <ref type="bibr" target="#b98">20,</ref><ref type="bibr" target="#b99">21</ref> gives us a good solution of this type.</p><p>MacKay assumed that the internal parameters of a model are randomly distributed, and the regularization parameters can be estimated by Bayesian inference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Typical machine learning models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Fully connected shallow &amp; deep neural network</head><p>The first-generation neural network, i.e. single layer perception <ref type="bibr" target="#b101">23</ref>      Shallow neural network is widely used in function approximation, nonlinear regression, pattern recognition and data clustering. It is a mature tool in materials research. The applications of it in materials science include: predicting physical properties <ref type="bibr" target="#b102">[24]</ref><ref type="bibr" target="#b103">[25]</ref><ref type="bibr" target="#b104">[26]</ref><ref type="bibr" target="#b105">[27]</ref><ref type="bibr" target="#b106">[28]</ref><ref type="bibr" target="#b107">[29]</ref> (e.g., stacking fault energy, phase transformation temperatures); mechanical properties <ref type="bibr" target="#b108">[30]</ref><ref type="bibr" target="#b109">[31]</ref><ref type="bibr" target="#b110">[32]</ref><ref type="bibr" target="#b111">[33]</ref><ref type="bibr" target="#b112">[34]</ref><ref type="bibr" target="#b113">[35]</ref> (e.g., strength, ductility, creep properties, fatigue properties); alloys optimization <ref type="bibr" target="#b114">[36]</ref><ref type="bibr" target="#b115">[37]</ref><ref type="bibr" target="#b116">[38]</ref><ref type="bibr" target="#b117">[39]</ref> ;</p><p>facilitating quality control <ref type="bibr" target="#b118">[40]</ref><ref type="bibr" target="#b119">[41]</ref><ref type="bibr" target="#b120">[42]</ref><ref type="bibr" target="#b121">[43]</ref><ref type="bibr" target="#b122">[44]</ref><ref type="bibr" target="#b123">[45]</ref><ref type="bibr" target="#b124">[46]</ref><ref type="bibr" target="#b125">[47]</ref> (e.g., discriminating weld defect type, forecasting weld pool shape in welding and mold level control in continuous caster). The ISIJ special issue of neural network 48 and Bhadeshia's works <ref type="bibr" target="#b95">17,</ref><ref type="bibr" target="#b127">49</ref> show us more examples of the applications of shallow neural networks in material science. Researchers believe that the power of a neural network increases with its depth.</p><p>Bengio <ref type="bibr" target="#b128">50</ref>  reduce the number of parameters but keep important information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Decision tree</head><p>Tree-based machine learning models are a family of non-parametric supervised methods which are widely used in materials science <ref type="bibr" target="#b135">[57]</ref><ref type="bibr" target="#b136">[58]</ref><ref type="bibr" target="#b137">[59]</ref><ref type="bibr" target="#b138">[60]</ref> due to their good interpretability. A decision tree 1 looks like an upside-down tree, with the first decision rule at the top and following decision rules spreading out below. In a decision tree, every decision rule (e.g., </p><formula xml:id="formula_3">"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘˜</head><p>The sum is extended to all nodes where xi is used, and Nk is the number of samples reaching the node k, âˆ†ğ¼ ğ‘¥ ğ‘– is the decrease of impurity index. Therefore, the importance is a weighted sum of all impurity reductions computed considering only the nodes where the feature is used to split them.</p><p>When decision trees are built, many of the branches may reflect noise or outliers in the training data. Tree pruning attempts to identify and remove such branches to improve model's generalization. Linear kernel, radial basis function (RBF) kernel, and polynomial kernel are frequently used in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Support vector machine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Ensemble and bagging</head><p>The models introduced previously are all so-called strong learners which solve a specific problem by looking for the best possible solution. Another approach is based on a set of weak learners that can be trained in parallel or sequentially (with slight modifications on the parameters) and used as an ensemble based on a majority vote or the averaging of results <ref type="bibr">2,</ref><ref type="bibr" target="#b100">22</ref> . These methods can be classified into two main categories: bagging (or bootstrap) and boost. In bagging, the ensemble is built completely. The training process is based on a random selection of the splits and the predictions are based on a majority vote. Random forests are an example of bagged tree ensembles. In boost, the ensemble is built sequentially, focusing on the samples that have been previously misclassified.</p><p>AdaBoost and gradient tree boosting are examples of boosted trees.  Transfer learning, where knowledge is shared between datasets and models, can be an effective tool for sparse and small datasets. Figure <ref type="figure" target="#fig_12">2</ref>.9 shows the architecture for single task and architectures for transfer learning based on conventional machine learning <ref type="bibr" target="#b139">61</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Transfer learning</head><p>Figure <ref type="figure" target="#fig_12">2</ref>.10 shows the architecture for the transfer learning technique based on convolutional neural network <ref type="bibr" target="#b140">62,</ref><ref type="bibr" target="#b141">63</ref> . The benefits of transfer learning include reducing training time, improving generalization with small datasets, sharing knowledge between related tasks, eliminating manual feature engineering. Machine learning was firstly designed and customized for its traditional application fields <ref type="bibr">4</ref> , e.g. recommendation system, computer vision, natural language processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Challenges in machine learning for materials research</head><p>However, the tasks in material science show different characteristics from typical machine learning applications. Thus, existing machine learning tools need adjustments to make it work in materials science. Table <ref type="table" target="#tab_10">2</ref>.1 compared machine learning for its traditional applications and machine learning for materials from the points of data, descriptors, and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Data-wise challenges</head><p>Big data is a phrase commonly associated with artificial intelligence and machine learning. Companies, e.g., Amazon, Google, Facebook, can quickly and cheaply gather big datasets of billions of samples. In materials science and engineering, the rate of generating data is very slow. In curation of material datasets, it is often completed by hand. The Materials algorithms project of Cambridge University, NIMS (Japan national institute for materials science) materials database provide dozens of materials datasets which are available only after decades of accumulation. Materials genome initiative (MGI) <ref type="bibr" target="#b142">[64]</ref><ref type="bibr" target="#b143">[65]</ref><ref type="bibr" target="#b144">[66]</ref> , high-throughput computing <ref type="bibr">[67]</ref><ref type="bibr" target="#b146">[68]</ref><ref type="bibr" target="#b147">[69]</ref><ref type="bibr" target="#b148">[70]</ref><ref type="bibr" target="#b149">[71]</ref> and material chips have increased the speed of generating data by dozens to hundreds of folds. However, it is still impossible to assemble big datasets like that in the internet and e-commerce. Thus, datasets in materials fields are often small (datasets usually have dozens of examples to several thousand of examples) for a specific task <ref type="bibr" target="#b150">72,</ref><ref type="bibr" target="#b151">73</ref> .</p><p>Machine learning applications, e.g., product recommendation and consumer behavior analysis, acquire records from structured sources like website analytics tools. Images and videos used in computer vision can easily transform file formats. Materials data come from different sources (e.g., test results, simulation data, handbook data, supplier datasheets) and in different formats (e.g., microstructure images, datasheets of composition/processing/properties, processing history records, spectrum). Thus, material datasets are often highly heterogeneous. Curating a reliable dataset from distinct data sources requires deep understanding and careful processing of the data.</p><p>Sample bias in datasets is common in materials science. Failed experiments and materials of poor properties are commonly discarded and cannot be accessed publicly.</p><p>The biased distribution of data is detrimental to the generalization of the built machine learning models without doubt.</p><p>The error and variation from processing and measurement are inevitable in material data.</p><p>How to take into account of their influence on the achieved models is a big challenge for researchers in applying machine learning to material problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Descriptor-wise challenges</head><p>With a set of suitable descriptors, conventional machine learning can perform very well even with small dataset <ref type="bibr" target="#b152">74</ref> . However, the optimal set of descriptors for a specific job in material research is not out-of-shelf. It is selected by trails and errors and adding new pertinent descriptors is always be considered if models' performance is not met requirement <ref type="bibr" target="#b153">75</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Task-wise challenges</head><p>Typical machine learning applications are tasks that are simple for humans but difficult to code explicitly, like speech recognition and image recognition. The tasks of material science and engineering are exploring the relationship between chemistry, processing, microstructure, properties, and performance; and to synthesis better materials by exploiting this knowledge. It is a task that is difficult for humans and hard to code explicitly. It demands prediction with high accuracy (even in extrapolation <ref type="bibr" target="#b154">76,</ref><ref type="bibr" target="#b155">77</ref> for materials scientists are more interested in developing materials of better performance)</p><p>and error estimation <ref type="bibr" target="#b156">[78]</ref><ref type="bibr" target="#b157">[79]</ref><ref type="bibr" target="#b158">[80]</ref><ref type="bibr" target="#b159">[81]</ref><ref type="bibr" target="#b160">[82]</ref> , models with interpretability 56,83-86 (a "black box" model cannot meet our requirements in the material field) and transferability. If a machine learning model cannot meet these requirements, its applications will be constrained.</p><p>Machine learning for materials science and engineering is more complicated than typical machine learning applications. It therefore requires a more sophisticated approach (special techniques and processes) at many points within the machine learning workflow.</p><p>In many typical machine learning applications, error estimation is not essential. For example, in the movies recommendation system, an error of 20% in prediction only means Â±1 point difference in a 0-5 scale. This usually has little impact on audiences.</p><p>However, a 20% error in a prediction of material properties could mean the prediction is unreliable, and we need to carry out more costly experiments. Error estimation help researchers understand predictions and decide the next step actions.</p><p>3 Literature review on solidification cracking &amp; phase formation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solidification cracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Hot cracking and solidification cracking</head><p>Hot cracking, which is also termed as hot tearing, hot shortness or hot brittleness, is one of the most severe defects formed at high temperature range in casting and welding, especially in Ni-based alloys and some Al-based alloys <ref type="bibr" target="#b165">87</ref> . Hot cracking can be defined as material separations that occur at high temperature range (above half the melting point of alloys) along the grain boundaries (dendrite boundaries) when the local strain and the local strain rate attain critical values <ref type="bibr" target="#b166">88</ref> . Hot cracking can be subdivided into three easilyconfused types: solidification cracking, liquation cracking and ductility dip cracking <ref type="bibr" target="#b167">[89]</ref><ref type="bibr" target="#b168">[90]</ref><ref type="bibr" target="#b169">[91]</ref> .</p><p>Ductility dip cracking is caused by reduced hot ductility of solid phase due to diffusion related compositional changes and precipitation of brittle secondary phases. Liquation cracking takes place in areas of liquation in heat-affected zone or in multi-pass welds.</p><p>Solidification cracking occurs in mush zone at the last stage of solidification. The fractography of solidification cracks exhibits dendrites characteristics. The solidification cracks can develop to be surface cracks or remain to be subsurface / internal cracks <ref type="bibr" target="#b167">89</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The complexity of solidification cracking</head><p>Solidification crack, which widely occurs in casting, welding, and additive manufacturing, is a very serious defect for it cannot be healed through following processing e.g. rolling,</p><p>forging and heat treatment. It appears at the last stage of solidification when liquid films exist between dendrites boundaries and the local strains cannot been accommodated through liquid phase feeding and solid phase deformation. It is a complex phenomenon for material, mechanical, and thermal factors work together to incur the nucleation and propagation of cracks and these factors interact with each other, see Figure <ref type="figure" target="#fig_48">3</ref>.1 <ref type="bibr" target="#b167">89</ref> . The weldability and castability of metallic materials are closely correlated with their solidification cracking susceptibility (SCS). The complexity and harmfulness of solidification cracking rationalize the existence of so many experimental techniques <ref type="bibr" target="#b170">92,</ref><ref type="bibr" target="#b171">93</ref> , physical models <ref type="bibr" target="#b172">[94]</ref><ref type="bibr" target="#b173">[95]</ref><ref type="bibr" target="#b174">[96]</ref> , and numerical models <ref type="bibr" target="#b175">[97]</ref><ref type="bibr" target="#b176">[98]</ref><ref type="bibr" target="#b177">[99]</ref> . Solidification cracking tests are time-consuming and costly. Thus, only very few alloys and parts can be tested. The difficulty of direct in-situ observation of solidification cracking phenomena caused by high temperature and opaque metallic matrix determines that indirect measurements are the main approaches of studying solidification cracking. Theories about solidification cracking susceptibility based either on metallurgical features, or strain, or strain rate, or stress criteria are either over-simplified (no existing theory can take account of all factors mentioned before) or lack solid physical basis thus cannot work effectively in practice.</p><p>Complex numerical models <ref type="bibr" target="#b175">97</ref> , which simultaneously modeled thermal, fluid, mechanical fields and intergranular network geometry, only able to predict the most susceptible positions of solidification cracking but not the solidification cracking susceptibility of alloys.    but in fact it is not. The cause might be relative to their high degree of coherency.</p><p>(7) Surface tension Solidification cracking involves the separation of liquid film between grains, so surface tension <ref type="bibr" target="#b165">87,</ref><ref type="bibr" target="#b167">89</ref> plays a role in the process. Alloys of bad wettability (i.e. high interface energy ğ›¾ğ¿/S and bridge between grains is strong) and very good wettability (i.e. low interface energy ğ›¾ğ¿/S and back-filling ability is high) are not solidification cracking susceptible.</p><p>Alloys of medium wettability are solidification cracking susceptible.</p><p>(8) Grain boundaries Grain boundaries <ref type="bibr" target="#b165">87</ref> are the areas where solidification cracking takes place. So the characteristics of grain boundaries have an obvious influence on solidification cracking susceptibility. These characteristics include grain shape (i.e. columnar vs equiaxed grains), grain structure (i.e. primary austenite stainless steel owns more straight grain boundaries and worse weldability than primary ferrite stainless steel) and grain size (fine grain means more grain interface need to be separated in the crack propagation compared with coarse grain. Thus fine grain castings and welds are less solidification cracking susceptible).</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9) Porosity</head><p>The formation of porosity <ref type="bibr" target="#b165">87,</ref><ref type="bibr" target="#b167">89</ref> is similar to the formation of solidification crack in that both involve the separation of liquid and the formation of new interface between liquid and vapor. So, it is very reasonable to correlate porosity with solidification crack, inferring the porosity can be the nucleus of solidification cracks. On the other hand, the formation of porosity reduces macro shrinkage, thus decrease solidification strain and solidification cracking susceptibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Mechanical factors (1) Strain</head><p>Pellini proposed the 'strain theory' of solidification cracking: solidification cracking occurs when the local strain beyond a critical strain. Based on the strain theory, Senda  The nucleation of a first void will take place when the local strain rate is larger than the critical deformation rate (ğœ€á¹—,ğ‘šğ‘ğ‘¥) <ref type="bibr" target="#b173">95</ref> .</p><p>(4) Restraints</p><p>Fixtures in weld and core in casting impose restraints on the mush zone in the process of solidification. Restraints can evolve into local strain/stress which affects solidification cracking susceptibility. Castings and welds of some structure, e.g. dog-bone shape workpieces, give themselves self-restraint 104 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Thermal factors</head><p>Process parameters e.g., melt temperature (overheat), mold materials and preheat temperature, cooling conditions (e.g., air cooling, furnace cooling) in casting, and the type of heat source (e.g., laser, electrical arc, flame), input power (welding current and welding voltage), welding speed in welding are thermal factors that determine temperature filed and temperature gradient field which influence the solidification cracking susceptibility with metallurgical and mechanical factors together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Solidification cracking of stainless steel</head><p>Stainless steel is an important type of steels which is widely used in corrosion, marine, high temperature and cryogenic environment and always suffer solidification cracking in processing. Commercial stainless steel commonly contains at least a dozen elements. Its major elements include Fe, Ni, Cr, Si, Mn, Mo; and its minor elements include C, N, P, S, Ti, Nb, B. The phases in stainless steel include matrix e.g., ferrite, austenite and martensite; and secondary phases e.g., carbides, nitrides, sulfides and phosphides.</p><p>Composition variation causes changes in solidification mode, solidification sequence, elements segregation and secondary phases precipitation.</p><p>Ferrite can accept more impurity elements (like S and P) than austenite. Irregular ferrite/austenite grain boundary is not in favor of the propagation of cracking cracks.</p><p>Thus, a High ratio of Cr to Ni (or Cr equivalent to Ni equivalent) is good for solidification cracking resistance by forming a certain amount of ferrite like that in AISI 301 and 304.</p><p>Fully austenitic stainless steels like AISI 316 is more susceptible to solidification cracking than stainless steels contain a certain amount of ferrite like AISI 304.</p><p>Impurity elements C, N, P and S, which tends to segregate to grains boundaries, increase cracking susceptibility. S have a strong effect on interfacial energies and is added to stainless steels to increase Marangoni effect. The extent of increasing susceptibility with impurity elements are different among different steels.</p><p>The increase of solidification cracking susceptibility with the applied strain is a nonlinear behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Solidification cracking susceptibility test</head><p>Hundreds of test methods were proposed to qualitative and quantitatively evaluate the solidification cracking susceptibility <ref type="bibr" target="#b170">92,</ref><ref type="bibr">107</ref> of alloys. A controlled strain applied on a geometrically simple specimen is preferred for evaluation of cracking tendency, e.g.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8">Machine learning for solidification cracking</head><p>In order to find the relationship between solidification cracking susceptibility, composition and processing parameters, researchers of material science commonly need the bridges of atomic/mesoscopic/micro /macro structure analysis. They explore the relationship between structure, composition, and processing parameters as well as the relationship between structure and solidification cracking susceptibility, and a lot of new terminologies and models are proposed like the ratio of chromium equivalent to nickel equivalent, primary ferritic/austenitic solidification mode, impurities content, solidification region. Then an indirect relationship is built between them. But the main disadvantages of this method are the relationship found is always limited to specific compositions and processing parameters and most of them are qualitative descriptions.</p><p>A universal relationship is rarely found, especially for complex nonlinear phenomena.</p><p>Materials researchers use compositions, structures, and properties (which are called manual features in data science) to describe materials. The attractions of many databased methods, especially the deep learning, are in that the hierarchical structure of them can complete features engineering automatically, we can obtain solidification cracking susceptibility as function of composition and processing parameters, i.e., quantitative mappings from inputs to outputs, directly without the aids of structure analysis. Figure <ref type="figure" target="#fig_48">3</ref>.5 shows schematically the difference between traditional material science route and data science route.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phase formation prediction</head><p>Phases and their distribution in space determine the properties of a material. Predicting phase prototypes (crystal structures) of materials or phase formation in materials has been a fundamental task and a challenge for material science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Amorphous phase and glass-forming ability</head><p>Amorphous ribbon alloys (AMRs) and bulk metallic glass (BMGs) extended metallic materials from typical crystalline (CR) materials to amorphous state materials. Owing to their unique mechanical attributes were used as input. The glass-forming ability (crystalline/amorphous ribbon/bulk metallic glass), the critical casting diameter, and the supercooled liquid range were the output. They employed these machine learning models to optimize commercial Zr-based bulk metallic glasses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-principal element alloys</head><p>Multi-principal element alloys (MPEAs), which are also known as high-entropy alloys (HEAs), and concentrated solid solution alloys (CSSs), extended metallic materials from corner and edge regions to the center regions of multi-component phase diagrams <ref type="bibr">160- 162</ref> . Researchers attributed the formation of single-phase multi-principal element alloys to four core effects: high entropy, sluggish diffusion, high lattice distortion, and cocktail Conventional machine learning was also be explored to predict the phase formation in multi-principal element alloys 167-171 . enthalpy, and mixing entropy to build a neural network model to distinguish single-phase solid solution, amorphous structure and intermetallic compounds. On average, a predictive accuracy higher than 80% could be reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Phase prototypes of inorganic substances</head><p>Though density functional theory (DFT) and force field models have been successfully exploited to predict crystal structures of materials. Their applications are limited by computational time and cost. Recently, conventional machine learning models were also exploited to predict inorganic materials' structures.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Longitudinal varestraint test (LVT, see Figure <ref type="figure" target="#fig_48">3</ref>.4) is a reliable quantitative method for measuring solidification cracking susceptibility of alloys. Compared with other solidification cracking susceptibility tests, the amount of longitudinal varestraint test results in literature is significant. That is why we chose longitudinal varestraint test data in published literature to compile our solidification cracking susceptibility dataset.</p><p>Different researchers used different sample thickness, welding process parameters and applied strains in their tests, so it is difficult to compare results from different researchers and get a unified conclusion. However, it is not a big problem for machine learning: when we add new variables into dataset, the more variation in data space, the more opportunities for machine learning models to discover the hidden nonlinear relationships.</p><p>In this study, a small dataset including about 600 longitudinal varestraint test results on stainless steels was collected from dozens of published literatures 108,109,127-136,112,137-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data pre-process &amp; dataset division</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Data pre-process</head><p>The data used in shallow neural network and fully connected deep neural network were normalized before training. The input variables X were normalized between -0.5 and +0.5, as follows:</p><formula xml:id="formula_4">X ğ‘ = ğ‘‹ -ğ‘‹ ğ‘šğ‘–ğ‘› ğ‘‹ ğ‘šğ‘ğ‘¥ -ğ‘‹ ğ‘šğ‘–ğ‘› -0.5</formula><p>Where XN is the normalized value of X which has maximum and minimum values given by Xmax and Xmin respectively.</p><p>The target variables Y were normalized between -1 and +1, as follows:</p><formula xml:id="formula_5">ğ‘Œ ğ‘ = 2 * ğ‘Œ -ğ‘Œ ğ‘šğ‘–ğ‘› ğ‘Œ ğ‘šğ‘ğ‘¥ -ğ‘Œ ğ‘šğ‘–ğ‘› -1</formula><p>Where YN is the normalized value of Y which has maximum and minimum values given by Ymax and Ymin respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training &amp; testing datasets division</head><p>To achieve unbiased training/testing data division, all data were divided into groups first.</p><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine learning models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Fully connected shallow &amp; deep neural networks</head><p>The fully connected neural networks used in this study consisted of 21 input neurons (one neuron corresponds to one input variables), one to several hidden layers, a few hidden neurons (the neurons number is variable) in each hidden layer, and one output neurons. The transfer function of hidden layer is hyperbolic tangent function tanh(x), and the mathematical expression of the tanh(x) is as follows:</p><formula xml:id="formula_6">tanh(x) = ğ‘’ ğ‘¥ -ğ‘’ -ğ‘¥ ğ‘’ ğ‘¥ + ğ‘’ -ğ‘¥</formula><p>The transfer function of output layer is linear function y = x.</p><p>In this study, we tested shallow neural networks of neuron numbers from 1 to 35.</p><p>We also attempted three hidden   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Support vector machine</head><p>Support vector machine was also trained and tested in this study to validate the accuracy advantage of deep neural network. In this study, support vector machine regression with linear/Gaussian/ (order 2 and 3) polynomial kernels was used and the hyper-parameters of the support vector machine were optimized with Bayesian optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Tree-based models</head><p>Decision tree and random forest models were also trained and tested in this study to validate the accuracy advantage of deep neural network and to enhance the interpretability of our machine learning models (through analysing the relative importance of variables). The impurity criterion used in the tree-based models is 'Gini' index. We regulated the maximum depth of a tree (20 was used in our work) and the maximum decision tree number in the random forest (100 was used in our work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training details</head><p>Each support vector machine / shallow neural network / deep neural network configuration was trained more than 100 times using different random seeds. The optimal support vector machine, shallow neural network and deep neural network were chosen to make predictions and comparisons.</p><p>All machine learning works were carried out in Matlab 2018a with its statistics and machine learning toolbox and neural network toolbox. Due to small dataset and pretraining, the time required to train one deep neural network configuration with one random seed on a personal computer (@2.6GHz processor, 16GB RAM) varied from several seconds to dozens of seconds, which is comparable with that of shallow neural network and support vector machine and is obviously shorter than that of training deep neural network for image recognition where many GPUs and many hours or even many days are needed. The training time for tree-based models were quite short (only several seconds) and can be neglected.   This is consistent with other researchers' results: when we increase the neuron number in the hidden layer, the training accuracy of a neural network easily reaches very a high score e.g., 0.99, but the testing accuracy has a limit. That is one of the main reasons why researchers explore the deep neural network, for higher testing accuracy, which represents the real learning ability (the training accuracy can be perceived as the memory ability). The best training accuracy of shallow neural networks is 0.99, and the best testing accuracy is 0.89. So, the shallow neural network of 21-( <ref type="formula">21</ref>)-1 structure was chosen as the final optimal shallow neural network that was used for comparison in the next step.  also shows the testing accuracy of five hidden layers deep neural network is lower than that of four hidden layers deep neural network. This is not expected as deeper neural network should be more expressive and bring better performance according to the experience of deep neural network in image recognition, the possible cause of the behavior could be that the deep neural network structure is not optimal and the small dataset lacks enough training data to determine the optimal parameters of five hidden layers deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training and testing results</head><p>The training accuracy of three deep neural networks and shallow neural networks are close but higher than support vector machine. The four hidden layers deep neural networks of 21-(6-5-4-3)-1 structure shows the best testing accuracy 0.93, which is 0.04 higher in testing accuracy than that of the optimal support vector machine and shallow neural network i.e. 0.89.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Validation of solidification cracking susceptibility prediction</head><p>To further validate the generalization ability of the optimal deep neural network, their optimal models. We can see a better linear relationship between brittle temperature range and total crack length predictions by our optimal deep neural network (correlation coefficient R=0.944) than that of shallow neural network (R=0.830) and support vector machine (R=0.883). This further confirms that deep neural network has a higher generalization performance than shallow neural network and support vector machine.  Arata et al. <ref type="bibr">114</ref> reported that Mn addition to SUS310S with 0.002wt% P and 0.004wt% S exerts a harmful effect on cracking resistance under 4% applied-strain conditions using transverse varestraint test. On the other hand, under the same testing condition, Mn addition to SUS310S with 0.025wt% P and 0.007wt% S improves solidification cracking resistance. Prediction was carried out for the same SUS310S compositions published in Matsuda's paper. The two curves of Those validations using metallurgical experience confirm the correctness and generalization ability of our optimal deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Visualization of prediction and interpretability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Neural network equations</head><p>The mathematical expressions of the optimal one hidden layer shallow neural network of 21inputs-( <ref type="formula">21</ref>)-1output structure are listed as follows:</p><p>â„ ğ‘– <ref type="bibr" target="#b79">(1)</ref> = tanh ( âˆ‘ ğ‘Š ğ‘–ğ‘— <ref type="bibr" target="#b79">(1)</ref> ğ‘‹ ğ‘— + ğ‘ ğ‘–</p><p>) , ğ‘– = 1,2, â‹¯ ,21</p><formula xml:id="formula_8">ğ‘ = â„ 1 (2) = âˆ‘ ğ‘Š 1ğ‘— (2) ğ‘‹ ğ‘— + ğ‘ 1 (2) ğ‘—=21 ğ‘—=1</formula><p>where X is the normalized input vector, Z is the normalized output value, â„ ğ‘– (ğ‘˜) (k=1 and 2) is the output of the k th layer, ğ‘Š ğ‘–ğ‘— <ref type="bibr">(ğ‘˜)</ref> and ğ‘ ğ‘– <ref type="bibr">(ğ‘˜)</ref> are the weight and bias matrix of the k th layer.</p><p>The mathematical expressions of the four hidden layers 21inputs-(6-5-4-3)-1output</p><p>deep neural network are listed as follows:</p><p>â„ ğ‘– <ref type="bibr" target="#b79">(1)</ref> = tanh (âˆ‘ ğ‘Š ğ‘–ğ‘— <ref type="bibr" target="#b79">(1)</ref> ğ‘‹ ğ‘— + ğ‘ ğ‘–</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘—</head><p>) â„ ğ‘– (ğ‘˜) = tanh (âˆ‘ ğ‘Š ğ‘–ğ‘— (ğ‘˜) â„ ğ‘— (ğ‘˜-1) + ğ‘ ğ‘– (ğ‘˜) ) ğ‘— , ğ‘˜ = 2,3,4 ğ‘Œ = â„ ğ‘– <ref type="bibr">(5)</ref> = âˆ‘ ğ‘Š ğ‘–ğ‘— <ref type="bibr">(5)</ref> â„ ğ‘— <ref type="bibr">(4)</ref> + ğ‘ ğ‘– Combining neural network equations with the determined parameters together, the fitted functions can be easily used to estimate solidification cracking susceptibility dependence on compositions and strains (small strains can be utilized to predict solidification cracking susceptibility in casting and large strains can be chosen to predict solidification cracking susceptibility in welding and additive manufacturing) when the time consuming and expensive experimental results are not available, for example to select the optimal alloy composition, make comparison with experiment results, and so on. The following sections will provide several examples of applications. However, the applications of the machine learning models are not limited to those examples.         <ref type="table" target="#tab_0">4</ref>.2). It shows that solidification cracking susceptibility is susceptible to composition variation. An alloy with a small difference in the composition can lead to a great difference in solidification cracking behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Feature importance</head><p>(2) Solidification cracking susceptibility dependence on Ni and Cr    The prediction of 8 stainless steels' total crack length (an indicator of solidification cracking susceptibility) dependence on impurity elements P and S is shown in Figure <ref type="figure" target="#fig_21">4</ref>.16</p><p>(compositions are shown in Table <ref type="table" target="#tab_0">4</ref>.4). The tendency of increasing solidification cracking susceptibility with the increase of P and S is shown in the prediction. The extent of increasing susceptibility with P and S are different among different steels. The solidification cracking susceptibility of AISI 316 is very sensitive to the P and S contents.</p><p>Small difference in C and N contents can show great difference in solidification cracking behavior. But that of AISI 316L with less C content is less sensitive to the P and S contents.</p><p>The figure also shows that impurity elements P and S have a different impact on solidification cracking susceptibility (most contours are not symmetry along P=S diagonal), the commonly linear superposition P and S is improper.</p><p>(4) Solidification cracking susceptibility dependence on C and N  The prediction of 4 stainless steels' total crack length (an indicator of solidification cracking susceptibility) dependence on C and N is shown in Figure <ref type="figure" target="#fig_21">4</ref>.17 (5) Solidification cracking susceptibility dependence on strain The prediction of AISI 316 stainless steel's total crack length (an index of solidification cracking susceptibility) dependence on strain is shown in Figure <ref type="figure" target="#fig_21">4</ref>.18 (compositions are shown in Table <ref type="table" target="#tab_0">4</ref>.5). The tendency of increasing solidification cracking susceptibility with the increase of strain is correctly revealed in the prediction. This tendency is also a nonlinear behavior, which is very difficult for physical models to depict but is relatively easy for machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Summary</head><p>Five machine learning models, i.e., shallow neural network, fully connected deep neural network, support vector machine, decision tree and random forest, were used to predict solidification cracking susceptibility of stainless steels as a function of composition and processing parameters. Fully connected deep neural network models (with pre-training and fine-tuning) outperformed shallow neural network, fully connected deep neural network models (without pre-training), support vector machine and tree-based models in prediction accuracy; tree-based models have accepted accuracy and better interpretability than support vector machine and neural network models; the optimal deep neural network has more compact structure and less parameters than the optimal shallow neural network (thus able to reduce the risk of overfitting and increase generalization).</p><p>Through machine learning regression, vast scattered experimental data hidden in literature can produce simple quantitative expression: specific material property as function of chemistry composition and processing parameters. The derived models can be used in material property prediction, new alloys development, and comparison with experimental results.</p><p>The combination of multiple machine learning models shows that solidification cracking susceptibility of stainless steels was mainly determined by Ni content / Cr content (or ferrite content / austenite content), impurity content and the value of applied strain.</p><p>Though deep neural networks plus big datasets are the best choice, deep neural networks with small datasets and pre-training can be a reasonable choice when big datasets are not available. In material science, small datasets are common, and the problems to be solved have fewer input variables than that in image recognition. Thus, compact deep neural network (neuron number in each hidden layer is small) are suitable for material problems, such as solidification cracking susceptibility prediction in this study, and pre-training using stacked auto-encoder is effective and necessary in the fully connected deep neural network regression of small datasets. Our study demonstrates that compact / narrow fully connected deep neural network with a small dataset and</p><p>special training methods has huge potential for extensive applications in material study, especially for those multivariate nonlinear problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application of convolutional neural network &amp; periodic table representation to predicting glass-forming ability &amp; compound properties</head><p>The works of Sun 158 and Ward   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Glass-forming ability dataset</head><p>Enough reliable data is vital for the performance of a machine learning model. In this work, we try to assemble a glass-forming ability dataset as large as possible. Our dataset includes Sun's binary alloys dataset 158 (about 3000 entries) and Ward's ternary amorphous ribbon alloys dataset <ref type="bibr" target="#b136">58,</ref><ref type="bibr">159</ref> (about 6000 entries) and bulk metallic glasses dataset <ref type="bibr" target="#b135">57</ref> (about 800 entries) and Miracle's dataset 177 (about 300 entries). In those datasets, crystalline alloys data are in the minority, because amorphous ribbon alloys and bulk metallic glasses are the focus of research, crystalline compositions are commonly discarded and unpublished as the failed experimental results. In reality, the number of amorphous alloys is less than that of their crystalline counterparts. To compensate for this weakness and increase the variety of crystalline data in our dataset, we add 800 pieces of conventional crystalline metallic materials data (including steels, superalloys and Co, Al, Mg, Cu, Zn alloys) from https://www.makeitfrom.com/ into our dataset. After sorting and removing redundancy, our dataset has 10440 entries. Figure <ref type="figure" target="#fig_85">5</ref>.2 shows the statistics of our raw dataset. Our dataset contains 97 different elements in the periodic table, and many of these elements simultaneously present in entries of crystalline alloys (CR), amorphous ribbon alloys (AMR), and bulk metallic glasses (BMG).</p><p>The statistics of labels are shown in Table <ref type="table" target="#tab_3">5</ref>.1. The distribution of elements and three labels is good for data division and model training.    Though the dataset is not small, taking into consideration of the myriad of elemental combinations and compositional variation (e.g., if elements A, B, C, D vary from 1 to 99 at% with 1at% compositional interval, there will be 4851 possible combinations in one ternary system A-B-C alone; there will be 156849 possible combinations in one quaternary system A-B-C-D alone), the deficiency and sparsity of the data is apparent.</p><p>Alloys in our dataset only occupy a tiny part of the possible combinations, i.e., most compositional space is unexplored. This can be seen as another form of small dataset and is common when applying machine learning models to material problems.   Materials science community has made a great effort in accumulating big datasets of materials properties: e.g. open quantum materials database (OQMD) <ref type="bibr" target="#b146">68,</ref><ref type="bibr">178</ref> , automatic flow for materials discovery (AFLOWLIB) <ref type="bibr" target="#b148">70</ref> , the materials project <ref type="bibr" target="#b149">71</ref> , crystallography open database (COD) <ref type="bibr">179</ref> , Springer Materials (https://materials.springer.com/). These big datasets can be exploited as source datasets for transfer learning (just like the role of ImageNet dataset in computer vision field) to build the general and transferable feature extractors. In this work, we used a dataset of 228,676 compounds from OQMD which was also used by <ref type="bibr">Ward 58</ref> before. They are lowest-energy compounds at their unique compositions. The dataset involves these compounds' chemistry and their physical properties calculated by density functional theory (DFT). Convolutional neural networks are trained on the dataset to predict formation energy (Ef) and specific volume (V) of these compounds. Figure <ref type="figure" target="#fig_85">5</ref>.4 shows the elements distribution in the OQMD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Open quantum materials database dataset</head><p>Most elements in periodic table (89 out of 108) occur in the dataset, and 73 elements occur more than 8000 times. The good distribution of elements and the size of the source dataset is a guarantee of the effectiveness of general and transferable feature extractors. To exploit the convenience of convolutional neural network's automatic features extraction, we mapped (see Figure <ref type="figure" target="#fig_85">5</ref>.5) the raw data to 2-D images (increasing data dimensional) using a modified periodical table representation (PTR). We used the whole periodic right and from top to bottom according to the atomic number of elements (see Figure <ref type="figure" target="#fig_85">5</ref>.8). The preparation process is mapped to the last pixel in the atom table and the rest unused pixels are set to 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">2-D representation for data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Manual features engineering</head><p>The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Magpie descriptors</head><p>Ward et al. proposed a set of general-purpose descriptors, called Magpie descriptors <ref type="bibr" target="#b136">58</ref> , which fall into four categories:</p><p>(1) stoichiometric descriptors (6 in total) They are based on L p norms:</p><formula xml:id="formula_10">â€–ğ‘¥â€– ğ‘ = (âˆ‘ |ğ‘¥ ğ‘– | ğ‘ ğ‘› ğ‘–=0</formula><p>) 1/ğ‘ , p = 0, 2, 3, 5, 7 and 10 ğ‘¥ ğ‘– is the atomic fraction of the element i.</p><p>(2) Elemental-Property-Based Attributes (115 in total) They are based on statistics of the elemental properties (atomic number, Mendeleev number, column, row, atomic weight, melting temperature, covalent Radius, electronegativity, number of s valence electrons, number of p valence electrons, number of d valence electrons, number of f valence electrons, total number of valance Electrons, number of unfilled s states, number of unfilled p states, number of unfilled d states, number of unfilled f states, total number of unfilled states, specific volume of 0 K ground state, band gap energy of 0 K ground state, magnetic moment per atom of 0 K ground state, space group number of 0 K ground state). For each property, the minimum, maximum, range, fraction-weighted mean, average deviation, and mode (i.e. the property of the most prevalent element) of the values of the properties of each element present in the material is computed. The mean ğ‘“ Ì… and average deviation ğ‘“ Ì‚ are calculated as follows:</p><formula xml:id="formula_11">ğ‘“ Ì… = âˆ‘ ğ‘¥ ğ‘– ğ‘“ ğ‘– ğ‘“ Ì‚= âˆ‘ ğ‘¥ ğ‘– |ğ‘“ ğ‘– -ğ‘“ Ì… |</formula><p>where ğ‘“ ğ‘– is the property of element i, ğ‘¥ ğ‘– is the atomic fraction.</p><p>(3) Valance orbital occupation descriptors (4 in total)</p><p>The ratio of the average number of valance electrons in each orbital to the total number of valance electrons.</p><p>(4) Ionic compound descriptors (3 in total)</p><p>They are designed to determine whether a material is ionically bonded. The first measure is a Boolean denoting whether it possible to form an ionic compound. The other two attributes are based on the "ionic character" of a binary compound. The first attribute we used is the maximum ionic character between any two constituent elements in the material:</p><formula xml:id="formula_12">ğ¼(ğœ’ ğ‘– , ğœ’ ğ‘— ) = 1 -exp (-0.25(ğœ’ ğ‘– -ğœ’ ğ‘— ) 2 )</formula><p>where ğ¼(ğœ’ ğ‘– , ğœ’ ğ‘— ) is the fraction of ionic character, ğœ’ ğ‘– is the electronegativity of element i. The second attribute is the mean ionic character:</p><p>ğ¼ Ì… = âˆ‘ ğ‘¥ ğ‘– ğ‘¥ ğ‘— ğ¼(ğœ’ ğ‘– , ğœ’ ğ‘— )   <ref type="table" target="#tab_3">5</ref>.4 and Table <ref type="table" target="#tab_3">5</ref>.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">VGG-like convolutional neural network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training details</head><p>In predicting galss-forming ability, all models were created and tested using the Keras with Tensorflow as its backend. In the training phase, the output of the shallow neural network and convolutional neural network fitted the ground truth, and the categorical cross-entropy was used as the loss function to evaluate the fitness. The maximum epoch was set to 2000 (loss values almost remain unchanged), and 10-fold cross-validation (so ten models were created after training) was used to evaluate the training/testing accuracy. In prediction, the results of a committee consisted of 10 models were utilized.</p><p>A series of shallow neural networks using different input vectors of features and hyperparameters, e.g., neuron number in a hidden layer, were trained and optimized.</p><p>We found shallow neural networks of 30 neurons in the hidden layer did not show better accuracy than shallow neural networks of 20 neurons. Four shallow neural networks (with 20 neurons in the hidden layer) using different feature vectors achieved the best accuracies were compared with convolutional neural networks. All models were trained on a laptop with 2.6 GHz i7 processor and 16 GB memory. Training a convolutional neural network using random Initialization and AdaDelta algorithm on glass-forming ability dataset which had over 16k entries took about 10h under 10-fold cross-validation (batch size 64, 2000 epochs in total).</p><p>In prediction of compounds' forming energy Ef and specific volume V, all convolutional neural networkss were created and tested using Keras with Tensorflow as its backend. 80% dataset was used for training convolutional neural network and the rest 20% was kept out for testing to evaluate the performance of convolutional neural network. In the training phase, the output of the convolutional neural network fitted the ground truth, and the mean absolute error (MAE) was used as the loss function to evaluate the fitness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results of glass-forming ability prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Training and testing results</head><p>Table <ref type="table" target="#tab_3">5</ref>.6 shows the average training and testing accuracies of four shallow neural networks (SNNs) and three convolutional neural networks (CNNs) under 10-fold crossvalidation in predicting glass-forming ability. SNN1 (using only 14 features derived from empirical rules of metallic glass community) and SNN4 (using 145 general-purpose Magpie descriptors) show the lowest testing accuracy of about 90%. They show a marginal difference in accuracy with Ward's random forest model (89.9% vs. 90%). SNN4 and Ward's random forest model used 145 general-purpose Magpie features, and SNN1 only used 14 features (including one processing parameter, mixing entropy, the statistical information of atomic radius, Pauling electronegativity, bulk modulus, and work function). We found increasing features or even using the full list of features (see Table <ref type="table" target="#tab_3">5</ref>.3) did not bring noticeable improvement in accuracy. SNN2 only used one-hot composition vector as input, but it showed higher accuracy than SNN1 and SNN4. SNN3 used manual features vector plus composition vector as input and it improved the accuracy further. Due to our limited understanding of glass-forming ability and lacking precise property data as input (e.g., ideal solution model and Miedema model were used to calculate alloy mixing entropy and mixing enthalpy, respectively), improving the model's accuracy by adding more pertinent features is impracticable. All four shallow neural networks show lower accuracies than three convolutional neural networks.</p><p>Besides convolutional neural networks' accuracy advantage over shallow neural networks, it is also quite convenient to use convolutional neural networks. They only need compositions and processing parameters as input, and they automatically extract features through convolutional layers. CNN3 which refers to a convolutional neural network with periodic table representation shows the highest testing accuracy of 96.3%.</p><p>The only difference among the three convolutional neural networks is in that the data representations of CNN1 and CNN2 did not have periodic table structure. The advantage of CNN3 over CNN2 and CNN1 is not evident (only 1.3% higher). However, we will demonstrate that CNN3 has more obvious advantages over other models in predicting unseen alloys, i.e. better generalization.   All models (except SNN4) correctly predicted the five bulk metallic glasses in the ground truth and the predicted bulk metallic glasses cover certain area (not some discrete points) around the ground truth points. It is reasonable, researchers commonly reported the optimal bulk metallic glasses only, and bulk metallic glasses candidates (especially before the discovery of bulk metallic glasses) are archived as amorphous ribbon alloys. This sparse and ununiform distribution of bulk metallic glasses data points usually induces bulk metallic glasses data points buried by surrounding densely distributed amorphous ribbon alloys points and omitted as a noise, see Figure <ref type="figure" target="#fig_85">5</ref>.11(a). That is why we adjusted the ternary classification into binary classification, i.e., ternary classification easily underestimates alloys' glass-forming ability.  To validate the predicting ability of models on unseen alloy systems, we carried out a leave-one-system-out (LOSO, like the leave-one-cluster-out cross-validation used by Meredig et al) <ref type="bibr" target="#b154">76</ref>         In summary, when dataset is large enough (e.g. the Al-Ni-Zr system), though the benefit of adding domain expertise to representation is not obvious, the advantage of automatic feature engineering over manual feature engineering is evident; when data is rare or no data is available, domain expertise is vital. Periodic table structure plus CNN, like CNN3, not only brings the convenience of automatic feature engineering but also improves the generalization by introducing background knowledge. mean absolute error (MAE) for predicting Ef is 0.083eV/atom and for predicting V is 0.457 Ã… 3 /atom, which is better than Ward's result <ref type="bibr" target="#b136">58</ref> (MAE is 0.088 eV/atom and 0.563 Ã… 3 /atom for Ef and V, respectively) using the same dataset with 145 Magpie manual features plus random forest model. The results show convolutional neural network regression not only eliminates the need for manual features engineering but also improve the testing accuracy (generalization). We believe that feature extractors of these convolutional neural networks can extract a rich set of features (which is the basis for next step transfer learning) as these convolutional neural networks have been well trained (testing accuracy can be as high as 0.99) on a big dataset involving 228k substances and 89 elements.       knowledge and make a reasonable inference.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Leave-one-system-out cross-validation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results of compound properties prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">The benefits of periodic table representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Summary</head><p>Convolutional neural network can be easily used in predicting materials properties: mapping raw data, e.g. composition and processing parameters, to pseudo images with some special 2-D structure, e.g. periodic table; automatically extracting features and making classification/regression through convolutional neural network.</p><p>Case studies on glass-forming ability (GFA), compounds' formation energy (Ef) and specific volume (V) prediction show that: this method not only eliminated the need of the challenging manual features engineering, but also outperformed the known models based on manual feature engineering.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Phase prototype dataset</head><p>We selected phase prototypes data from 'Handbook of inorganic substance 2016' 189 . The handbook which has phase prototypes of more than 160k inorganic substance, belongs to the well-known Pauling files project 190 . In the Pauling files project, a phase prototype (crystal structures) was specified in three parts: type-defining compound (e.g., 'Fe', 'ZnS'),</p><p>Pearson symbol (e.g., 'cF4', 'tP8') and space group number (1 to 230). For example, the phase prototype of single-phase solid solution alloy 'Fe0.4Co0.2Ni0.      Vector Ef&amp;V , Vector Ef , Vector V , and one non-transfer-learning model using composition vector Vector comp .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualization of the automatically-extracted features</head><p>Challenges in predicting phase prototypes based on composition include small sample volume (small dataset) for half prototypes in our dataset, imbalanced data distribution (see Figure <ref type="figure" target="#fig_115">6</ref>.4 and Table <ref type="table" target="#tab_45">6</ref>.1), and large number of categorical labels (170 labels in our work, as far as we know in the published works 172 researchers discriminated no more than ten structures in a machine learning task). Manual feature engineering plus conventional machine learning were considered at the beginning. However, our dataset involves a variety of inorganic substances, e.g. solid solution alloys, metal intermetallics, oxides, sulfide, halide. It is quite challenging to build features manually (which require a deep understanding of these substances). Furthermore, some physical data, e.g. atomic radius, bulk modulus used in manual features engineering is unavailable for some elements in our dataset which is difficult to fill reliably by hand. For these reasons, we resorted to transfer learning.</p><p>To gain insight into the extracted features and explaining why transfer learning is effective, we attempted to visualize features vectors generated by the transferable feature extractors and to find some patterns from these visualizations. However, uncovering the relationship visually and intuitively from these high dimensional features is quite challenging. There will be a 192*17,762 matrix (i.e. one substance has a 192dimensional feature vector) for our phase prototypes dataset, and it necessities a dimensionality reduction of these features. We compressed these high dimensional features data by principal component analysis (PCA). We visualized the first 4 principal components to (partially) represent the multi-principal element alloy knowledge yielded by the well-trained feature extractors on glass-forming ability dataset. We visualized the first 3 principal components to (partially) represent the phase prototypes knowledge yielded by the well-trained feature extractors on OQMD dataset. The features of 355 multi-principal element alloys generated by glass-forming ability model were shown in Figure <ref type="figure" target="#fig_115">6</ref>.7. We can see that alloys of the same phases tend to cluster in the diagrams. Based on the first and second principal features, we can intuitively distinguish stable BCC, FCC, HCP and multi-phase alloys. Most alloys of metastable amorphous phases can be discriminated from other alloys of stable phases by the third and fourth principal features visually. It indicates transfer learning from glass-forming ability to multi-principal element alloys is successful and justifies the high scores of our model for multi-principal element alloys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Features analysis for multi-principal element alloys</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Features analysis for phase prototypes</head><p>Because it is impossible to illustrate the knowledge of all 170 prototypes and over 17k substances in a figure (even impossible in many figures due to the nature of a large number of high-dimensional vectors), we choose a few phase prototypes (some common prototypes and some uncommon prototypes) to illustrate their distribution in   Inorganic substances are colored according to their phase prototypes. The percentage represents the ratio of the variance on the principal axis direction.</p><p>Figure <ref type="figure" target="#fig_115">6</ref>.8 and Figure <ref type="figure" target="#fig_115">6</ref>.9 show that only using three principal components, i.e., three automatic generated principal features, we can discriminate thousands of substances belonging to dozens of prototypes. It indicates transfer learning from predicting Ef and V using OQMD dataset to predicting phase prototypes is successful and justifies the high scores of our model for prediction of phase prototypes. It can be concluded that basis of good application of transfer learning in this case is the sharing of mutual features between two domains, one with a large dataset and other with a small dataset. The root of mutual features can be attributed to the fundamental laws of physics and chemistry that all inorganic substances follow (i.e., different materials have some features in common). So, some general and mutual features exist that can be used for describing them, which is the basis of transfer learning. We should bear in mind that we have 384 features in total. Excluding some irrelevant features (i.e., features only relevant to predicting forming energy Ef and specific volume V), the rest are common features and generally useful in new applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Summary</head><p>Convolutional neural network cannot work well on small datasets for its internal parameters of a large number need to be determined through learning. Transfer learning can partially solve this challenge.</p><p>The feature extractors of convolutional neural networks that are well-trained on big datasets (e.g., OQMD, glass-forming ability) can be reused directly in new tasks to generate a rich set of general and effective features without the challenging manual feature engineering. In our study, transfer learning was effectively exploited to build a phase prototypes classifier which could accurately discriminate 170 phase prototypes and a phase classifier which could accurately distinguish BCC, FCC, HCP, amorphous, and multiple-phase mixture in multi-principal element alloys.</p><p>Transfer learning not only greatly decreases training time of new models, but also improves accuracy and generalization of new models with small datasets. In addition, periodic table knowledge embedded in data representations and knowledge shared between models is beneficial for transfer learning tasks with small datasets. It can be easily used to create machine learning models quickly with small datasets and limited domain expertise for new materials designing, screening and development.</p><p>7 Conclusions and future work The proposed three deep learning techniques is an upgrade for conventional machine learning and could serve as an effective guide for the development of new materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future work</head><p>This thesis mainly deals with the challenges of small datasets and manual feature engineering when applying machine learning to materials problems. Future works are listed as follows.</p><p>There are many alternatives to stacked autoencoder in pre-training deep neural network, e.g. restricted Boltzmann machine and some empirical equations. Those should be tried and compared in future and some guidelines in training compact deep neural networks with small datasets can be summarized. In this thesis, the uncertainty of predictions was not given due to our datasets came from multiple sources and had different fidelities. In this scenario, how to estimate the prediction error/uncertainty proposes a big challenge for machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Periodic table representation is</head><p>We partially solved the problems of unbalanced/biased labels distribution in the glassforming ability dataset through transforming ternary classification to binary classification. However, biased sampling is common in materials science. How to assign each sample with different weight (importance) is another challenge in machine learning for materials science.</p><p>How to gain more insights into machine learning models (improve models' interpretability) is a big challenge for machine learning. Materials researchers need more tools to interpret their models and to understand the physical mechanisms of materials behaviors (transform black-box models into grey-box models).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Table 2 . 1 15 Table 4 . 1 43 Table 4 . 2</head><label>2115414342</label><figDesc>Challenges in machine learning for materials research. ............................................................. The best testing accuracies of five models used in prediction. .................................................. The specifications (wt%) of 304 and 310S stainless steel and the testing parameters used in prediction. .......................................................................................................................................... 53</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>75 Table 5 . 6 77 Table 5 . 7 80 Table 5 . 8</head><label>755677578058</label><figDesc>), CNN2 and CNN3 are assigned with the same hyperparameters. ..................................................................................................................... 74 Table 5.5 The summary of our VGG-like convolutional neural network used in the prediction of compounds' forming energy Ef and specific volume V. .......................................................................................... Comparison of average accuracy among different models under 10-fold cross-validation in predicting glass-forming ability . ........................................................................................................ Comparison of models' prediction accuracy on unseen alloy systems. ..................................... Comparison of experimental results with the predictions by CNN1, CNN2, CNN3, SNN3, and SNN4 in Ir-Ni-Ta-(B), Mg-Cu-Yb alloys. The red indicates the wrong prediction, and the green indicates the right prediction. ................................................................................................................................. 82 Table 5.9 Comparison of experimentally results with the predictions by CNN1, CNN2, CNN3, SNN3, and SNN4 in sulfur-bearing alloys. The red indicates the wrong prediction, and the green indicates the right prediction. ................................................................................................................................. 83 Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>........................................................................................................................................................... 85 Table 5.11 Comparison of experimentally measured glass-forming ability with the predictions by CNN1, CNN2, CNN3, SNN3 and SNN4 in 18 binary outliers. The red indicates the wrong prediction, and the green indicates the right prediction. Superscript GT denotes this composition is in our dataset; superscript * denotes this composition is not in our dataset, but some other compositions of this alloy system are in our dataset. ......................................................................................................... 85 Table 6.1 Prototype names and sample numbers ...................................................................................... 99 Table 6.2 The scores of transfer learning on multi-principal element alloys dataset under 5-fold crossvalidation with three data representations ..................................................................................... 103 Table 6.3 Confusion matrices of transfer learning models for predicting multi-principal element alloy phases on testing datasets under 5-fold cross-validation. ............................................................... 104 Table 6.4 The performance metrics of transfer learning models for predicting multi-principal element alloy phases under 5-fold cross-validation ............................................................................................... 105</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 . 1</head><label>11</label><figDesc>Figure 1.1 The structure of this thesis. ........................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 1</head><label>21</label><figDesc>Figure 2.1 Schematic diagram for a parametric machine learning system, Î¸ represents parameter vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 2</head><label>22</label><figDesc>Figure 2.2 Workflow for (a) conventional machine learning, (b) deep learning, (c) transfer learning.........</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 . 3</head><label>23</label><figDesc>Figure 2.3 Schematic diagrams for (a) best fit, (b) underfit, (c) overfit, and (d) training error/ testing error vs. model complexity 17 . ........................................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 . 4</head><label>24</label><figDesc>Figure 2.4 Typical algorithms in machine learning. ......................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 . 5</head><label>25</label><figDesc>Figure 2.5 Schematic diagram for a neuron, the basic unit of a neural network. ........................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 . 6</head><label>26</label><figDesc>Figure 2.6 Schematic diagrams of one hidden layer fully connected shallow neural network and j hidden layers fully connected deep neural network. .....................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 . 7</head><label>27</label><figDesc>Figure 2.7 Schematic diagram for the architecture differences between shallow network and deep network for the same task. ..............................................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 . 8 A</head><label>28</label><figDesc>Figure 2.8 A schematic diagram for support vector machine. ...................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.9 (a) Single task architecture and (b-d) architectures for three transfer learning techniques based on conventional machine learning 61 . .................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.10 Architecture for the transfer learning technique based on convolutional neural network. ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 . 1</head><label>31</label><figDesc>Figure 3.1 Schematic diagram for the complex interaction between thermal, material, mechanical factors, which cause solidification cracking. ...................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 . 2</head><label>32</label><figDesc>Figure 3.2 Stages of dendrites growth 95 ....................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 3 . 3</head><label>33</label><figDesc>Figure 3.3 Ductility curve according to Senda's theory 89 ...........................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 3 . 4</head><label>34</label><figDesc>Figure 3.4 Schematic diagram for longitudinal varestraint test 108 . ............................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 3 . 5</head><label>35</label><figDesc>Figure 3.5 Comparison of physical-based methods and data-based methods for a materials science problem. The goal of material research is to determine the relationship between material property, composition, and processing. To achieve the goals, traditional material science completed it manually by structure analysis and computer modeling, modern data science can complete it automatically through mining the relationship between X and Y by machine learning algorithms. ........................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 4 . 1</head><label>41</label><figDesc>Figure 4.1 The workflow of predicting solidification cracking susceptibility using deep neural network (DNN). ................................................................................................................................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 4 . 2 37 Figure 4 . 3 40 Figure 4 . 4 42 Figure 4 . 5</head><label>42374340444245</label><figDesc>Figure 4.2 Histograms of the 22 variables in the dataset (487 samples in total). Mean, minimum, maximum, std are also shown in the histograms................................................................................................. 37 Figure 4.3 Scatter matrix of total crack length (TCL) and other 21 variables, the values of Pearson correlation coefficients Rs are also shown in the figures. .................................................................. 40 Figure 4.4 Schematic diagram of deep neural network initiation by stacked auto-encoder. ..................... 42 Figure 4.5 The regression analysis on the unseen testing dataset between experimental data (horizontal axis, T values) and neural network predictions (vertical axis, Y values). The black dotted line is Y=T. Pearson correlation coefficients Rs are also shown in the figures. (a) deep neural network's predictions, (b) shallow neural network's predictions, (c) support vector machine's predictions, (d) random forest's predictions, and (e) decision tree's predictions. ...................................................... 44</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.6 (a) Shallow neural network's training and testing accuracies as a function of neuron number. (b) Neural networks' training and testing accuracies as a function of their depth. The support vector machine's accuracies are also shown in figure. ................................................................................. 45</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 4 . 7 47 Figure 4 . 8</head><label>474748</label><figDesc>Figure 4.7 Regression analysis between total crack length predictions and brittle temperature range experimental results. (a) predictions of support vector machine, (b) predictions of shallow neural network and (c) predictions of deep neural network. ....................................................................... 47 Figure 4.8 Predicted total crack length as functions of (a) Mn, (b) Ti, (c) N, (d) Si, the compositions used in prediction are shown in the figures. .................................................................................................. 48</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 4 . 50 Figure 4 . 51 Figure 4 .</head><label>4504514</label><figDesc>Figure 4.9 (a) The optimal shallow neural network's structure; (b) the optimal deep neural network's structure. ........................................................................................................................................... 50 Figure 4.10 The weights and biases tensors of the optimal 21-(21)-1 shallow neural network. ............... 51 Figure 4.11 The weights and biases tensors of the optimal 21-(6-5-4-3)-1 fully connected deep neural network. ............................................................................................................................................. 51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.12 A decision tree for predicting solidification cracking susceptibility. ....................................... 51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 4 . 52 Figure 4 .</head><label>4524</label><figDesc>Figure 4.13 The feature importance of solidification cracking susceptibility derived from the random forest model. ................................................................................................................................................ 52 Figure 4.14 Total crack lengths of 304 and 310S stainless steel vary with strain, Cr, Ni, P, S contents vary in the specification. ................................................................................................................................ 53</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.15 The prediction of 8 stainless steels' total crack length (mm) dependence on Ni and Cr (their Ni and Cr contents are in AISI specification), compositions used in predictions are shown in Table 4.3,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>57 Figure 4 . 58 Figure 4 . 59 Figure 5 . 1 63 Figure 5 . 2 65 Figure 5 . 3 65 Figure 5 . 4 67 Figure 5 . 5 68 Figure 5 . 6 69 Figure 5 . 7 69 Figure 5 . 8 70 Figure 5 . 9 73 Figure 5 . 73 Figure 5 . 79 Figure 5 . 80 Figure 5 . 84 Figure 5 . 87 Figure 5 . 87 Figure 5 . 89 Figure 5 . 89 Figure 5 . 90 Figure 5 . 91 Figure 5 . 93 Figure 6 . 1 95 Figure 6 . 2 97 Figure 6 . 3 98 Figure 6 . 4 98 Figure 6 . 5 Figure 6 . 6 Figure 6 . 7 Figure 6 . 8 Figure 6 . 9</head><label>5745845951635265536554675568566957695870597357357958058458758758958959059159361956297639864986566676869</label><figDesc>Figure 4.17 The prediction of stainless steels' total crack length (mm) dependence on C and N, compositions used in predictions are shown in Table 4.5. Processing parameters are the same: Th=3.18 mm, I=100 A, U=12 V, Ve=4.23 mm/s, strain=3%. ................................................................ 58 Figure 4.18 The prediction of AISI 316 stainless steel's total crack length (mm) dependence on strain with C and N vary in specification, processing parameters are: Th=3.18 mm, I=100 A, U=12 V, Ve=4.23 mm/s. ................................................................................................................................................. 59 Figure 5.1 The workflow of predicting glass-forming ability using convolutional neural network plus periodic table representation, MF, PTR and CNN denote manual features, periodic table representation and convolutional neural network, respectively. ...................................................... 63 Figure 5.2 The statistics of elements distribution in our dataset. Many elements appear in crystalline alloys (CR), amorphous ribbon alloys (AMR), and bulk metallic glasses (BMG) simultaneously. ................. 65 Figure 5.3 The statistics of alloy systems' size in our dataset. Most alloy systems (918 out of 1134) have nine entries or less, the deficiency and sparsity of the data are apparent. ....................................... 65 Figure 5.4 Elements distribution in OQMD dataset. The occurrence numbers of elements in the dataset are given under element symbols. The white squares, i.e. square with element symbol and without occurrence number, indicate these elements are not in the dataset. ............................................... 67 Figure 5.5 Mapping composition to 2-D representation. ........................................................................... 68 Figure 5.6 Schematic diagram for periodic table representation used in CNN3. ....................................... 69 Figure 5.7 Schematic diagram for randomized periodic table representation used in CNN2. ................... 69 Figure 5.8 Schematic diagram for atom table representation used in CNN1............................................. 70 Figure 5.9 Schematic diagram for our VGG-like convolutional neural network for glass-forming ability prediction. .......................................................................................................................................... 73</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.1 shows the structure of this thesis. Chapter 1 introduces the basics of machine learning and the challenges in machine learning for materials problems. Chapter 2 summarizes the theory of solidification cracking and phase formation, the applications of machine learning to predicting them in literature. Chapter 3 presents our attempts to improving model performance with a small dataset through deep neural network and pre-training. Case study about solidification cracking susceptibility prediction was used as a successful example. Chapter 4 presents our attempts to bypass the tricky manual feature engineering through periodic table representation and convolutional neural network. Case studies about predictions of glass-forming ability and compounds' properties (formation energy and specific volume) were used as evidences. Chapter 5 presents our attempts to tackle manual feature engineering and small datasets through deep transfer learning (with the help of the well-trained models in Chapter 4). Case studies about predictions of phase formation in multi-principal element alloys and compounds' phase prototypes were used as two successful examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 1 . 1</head><label>11</label><figDesc>Figure 1.1 The structure of this thesis.</figDesc><graphic coords="21,85.70,99.25,670.35,388.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.1 shows the schematics for a parametric machine learning system (e.g., logistic regression, naive Bayes, neural network): machine learning algorithms approximate the given dataset using a set of base functions, and the obtained surrogate models are used in new example prediction. A generic parametric training process must find the best parameter vector which minimizes the regression/classification error given a specific training dataset and it should also generate a model that can correctly generalize when unknown samples are provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 2 . 1</head><label>21</label><figDesc>Figure 2.1 Schematic diagram for a parametric machine learning system, Î¸ represents parameter vector.</figDesc><graphic coords="23,99.25,72.00,425.06,119.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 2 . 2</head><label>22</label><figDesc>Figure 2.2 Workflow for (a) conventional machine learning, (b) deep learning, (c) transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.2 shows the workflows for conventional machine learning, deep learning, and transfer learning. Data preprocess involves denoise, token, filling missing data. Feature extraction includes some standard processing e.g., scale invariant feature transform (SIFT) and some special processing based on domain expertise and empirical knowledge.</figDesc><graphic coords="23,99.25,283.24,425.13,217.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 2 . 3</head><label>23</label><figDesc>Figure 2.3 Schematic diagrams for (a) best fit, (b) underfit, (c) overfit, and (d) training error/ testing error vs. model complexity 17 .</figDesc><graphic coords="25,99.25,105.98,423.63,357.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 2 . 4</head><label>24</label><figDesc>Figure 2.4 Typical algorithms in machine learning.</figDesc><graphic coords="27,99.25,175.55,425.15,250.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.4 shows some typical algorithms in machine leaning 1,2,22 . Neural network, decision tree, nearest neighbor, NaÃ¯ve Bayes, support vector machine are usually used in data classification. Neural network, Gaussian process, linear regression are usually used in data regression. Neural network, hidden Markov model, self-organizing map (SOM), K-nearest neighbors (KNN), K-means are usually used in data clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 2 . 5</head><label>25</label><figDesc>Figure 2.5 Schematic diagram for a neuron, the basic unit of a neural network.</figDesc><graphic coords="28,154.80,72.00,314.04,188.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.5 shows the basic unit of a neural network (i.e. a neuron): the input vector [p1, p2, â€¦, pR] are multiplied by corresponding weights [w1,1, w1,2, â€¦ w1,R] and their summation (include a bias b, a special weight with constant input value 1) z is: z = ğ‘¤1,1ğ‘1+ğ‘¤1,2ğ‘2+ â‹¯ +ğ‘¤1,ğ‘…ğ‘ğ‘…+ğ‘*1. The mathematical expression can be abbreviated by matrix notation: n = ğ‘Š1Ã—ğ‘…ğ‘ğ‘…Ã—1+ğ‘, where ğ‘Š1Ã—ğ‘… is the 1 row and R columns weight matrix, and ğ‘ğ‘…Ã—1 is the R row and 1 column input vector. Then n is processed by the transfer function f(z) and it yields the neuron's output a: a = f(z) = f(Wp+b) = f(w1,1p1+w1,2p2+â€¦+w1,RpR+b). Based on the unit, we can build a complex neural network (multiple neurons, multiple layers, and multiple inputs) through increasing the number of neurons and layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.6 shows a shallow neural network of one hidden layer and a deep neural network of j hidden layers schematically. The X represents input vector, Y represents output vector. The number ni in brackets signifies the number of neurons in the i-th hidden layer. Circles represent neurons and lines represent the links between them, W j represents the weights matrix of the j-th layer. The biases are omitted for simplicity. The structures of deep neural network and shallow neural network are similar, except that deep neural network has more layers and a more obvious hierarchy structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 2 . 6</head><label>26</label><figDesc>Figure 2.6 Schematic diagrams of one hidden layer fully connected shallow neural network and j hidden layers fully connected deep neural network.</figDesc><graphic coords="29,99.25,72.00,425.15,156.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 2 . 7</head><label>27</label><figDesc>Figure 2.7 Schematic diagram for the architecture differences between shallow network and deep network for the same task.</figDesc><graphic coords="29,154.80,491.51,314.05,218.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 2 . 8 AFigure 2 . 8 .</head><label>2828</label><figDesc>Figure 2.8 A schematic diagram for support vector machine.Support vector machine 2 (SVM) accomplishes classification tasks by creating a hyperplane to segregate the data. This hyperplane is determined by maximizing the vector normal to the hyperplane (usually labelled W) and the closest data point to create the largest gap possible. A schematic diagram for support vector machine is shown in Figure2.8. When working with nonlinear problems, it is useful to map original vectors into a higher dimensional space where they can be linearly separated. The so-called kernel trick 2 realizes this nonlinear transformation. The value of the kernel for two feature vectors is the product of the two projected vectors: ğ¾(ğ‘¥Ì… ğ‘– , ğ‘¥Ì… ğ‘— ) = ğœ™(ğ‘¥Ì… ğ‘– ) ğ‘‡ ğœ™(ğ‘¥Ì… ğ‘— ).</figDesc><graphic coords="32,141.57,105.97,340.13,225.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.9 (a) Single task architecture and (b-d) architectures for three transfer learning techniques based on conventional machine learning 61 .</figDesc><graphic coords="33,99.25,197.53,425.13,265.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.10 Architecture for the transfer learning technique based on convolutional neural network.</figDesc><graphic coords="33,99.25,522.73,425.15,163.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 3 . 1</head><label>31</label><figDesc>Figure 3.1 Schematic diagram for the complex interaction between thermal, material, mechanical factors, which cause solidification cracking.</figDesc><graphic coords="39,99.25,319.70,425.10,214.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>3. 1 . 3 ( 3 )</head><label>133</label><figDesc>Material factors(1) Solidification interval Solidification interval or solidification temperature range<ref type="bibr" target="#b167">89</ref> Î”T of an alloy is defined as Î”T=TL-TS or Î”T= TL-TEut, where TL is liquidus temperature, TS is solidus temperature (use eutectic temperature TEut instead in nonequilibrium solidification). Solidification cracking occurs during liquid transforming to solid. Impurity elements such as sulfur and phosphorus remarkably enlarge the solidification interval of Fe-based and Ni-based alloys by forming low melting point eutectics e.g. FeS-Fe (TEut = 985Â°C), Fe3P-Fe (TEut = 1050Â°C), NiS-Ni (TEut = 644Â°C), and Ni3P-Fe (TEut = 880Â°C). Thus, some researchers correlated solidification interval with solidification cracking susceptibility and proposed that solidification cracking susceptibility is in proportion to the solidification interval.However, many solidification cracking phenomena cannot be explained using this simple criterion, even use the solidification interval modified by Scheil model and back-diffusion model.(2) Vulnerable Region Clyne and Davies defined solidification cracking sensitivity (SCS)<ref type="bibr" target="#b165">87</ref> based on thermal analysis: SCS = ğ‘¡ğ‘£/ğ‘¡ğ‘Ÿ, where ğ‘¡ğ‘£ is the vulnerable time period during which local strain in mush zone is difficult to be accommodated and cracks can easily propagate, and ğ‘¡ğ‘Ÿ is the immune time period during which local stress and strain can be easily relaxed through liquid flow and solid deformation. The biggest problem of Clyne's solidification cracking sensitivity criterion is they simply choose (without much reason) ğ‘¡ğ‘£ and ğ‘¡ğ‘Ÿ as: ğ‘¡ğ‘£ = ğ‘¡0.99-ğ‘¡0.90 , ğ‘¡ğ‘Ÿ = ğ‘¡0.90-ğ‘¡0.40, where the subscript numbers represent the volume fraction of solid. Different researchers may choose different ğ‘¡ğ‘£ and ğ‘¡ğ‘Ÿ region, to make prediction results fit experiment results better. Solidification cracking temperature range (SCTR) In varestraint test, the maximum crack distances (MCD) are quantified through imposing augmented strain on samples. If the cooling rate is tested by inserting a thermocouple into weld pool, a characteristic temperature range which is called solidification cracking temperature range (SCTR) 100 is defined as follow: SCTR=[Cooling rate]Ã—[MCD/V], where V is the welding speed (m/s). SCTR is a useful parameter when comparing the relative weldability between different alloys. (4) Back-filling Solidification shrinkage causes shrinkage porosity 87 . Porosity is a favorable site for solidification cracking initiation. Back-filling can reduce solidification porosity and heal solidification cracks. Risers are designed in casting to mitigate or eliminate solidification cracks, and the weld pool in welding plays the same role. The back-filling ability is closely connected with solidification cracking susceptibility. The back-filling ability is determined by dendrite tortuosity, melt viscosity, interface energy between melt and solid, and solid fraction. (5) Dendrite coherency The difficulty of bridging or coalescence of dendrite arms during the last stage of solidification plays an important role in solidification cracking 101 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 3 . 2</head><label>32</label><figDesc>Figure 3.2 Stages of dendrites growth 95</figDesc><graphic coords="41,99.25,199.86,414.25,122.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.2 shows different stages that dendrites experience in directional solidification growth 95 : in stage (a -b) solid fraction increases with time but liquid back-filling is still available, dendrites begin to touch each other and solid bridges start to form at the end of this stage; in stage (c) only continuous liquid films exist between dendritic arms, it is the solidification cracking susceptible region; in stage (d-f) discontinuous residual liquiddecreases and vanish at last, in this stage macro stain and stress gradually appear. Two characteristic temperatures 102-104 , i.e. coherency temperature and rigidity temperature, separate the three phases. Coherency temperature is the temperature that dendrites begin to touch each other. Rigidity temperature is the temperature that dendrites begin to be able to withstand tensile stress. During coherency temperature and rigidity temperature, tensile stress in mush zone will induce liquid film rupturing and cracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head></head><label></label><figDesc>et al. proposed the conception of 'ductility curve' (shown in Figure3.3) which can be determined by Gleeble experiment and other methods<ref type="bibr" target="#b167">89,</ref>100,106  .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 3 . 3</head><label>33</label><figDesc>Figure 3.3 Ductility curve according to Senda's theory 89</figDesc><graphic coords="43,205.32,121.96,211.91,202.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head></head><label></label><figDesc>threshold and brittle temperature range (BTR) are measured as the index for solidification cracking susceptibility. A large amount of longitudinal varestraint test results are available in literature 108,109,127-136,112,137-139,120-126 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>Figure 3 . 4</head><label>34</label><figDesc>Figure 3.4 Schematic diagram for longitudinal varestraint test 108 .</figDesc><graphic coords="46,129.57,72.00,364.03,450.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 3 . 5</head><label>35</label><figDesc>Figure 3.5 Comparison of physical-based methods and data-based methods for a materials science problem. The goal of material research is to determine the relationship between material property, composition, and processing. To achieve the goals, traditional material science completed it manually by structure analysis and computer modeling, modern data science can complete it automatically through mining the relationship between X and Y by machine learning algorithms.</figDesc><graphic coords="47,148.13,341.67,327.35,277.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head></head><label></label><figDesc>effects. There are challenges in understanding the evolution of phases in multi-principal element alloys. Early work on multi-principal element alloys was mainly focused on 3-d transition metals. The so-called Cantor alloy containing FeCrMnNiCo showed high strength and high ductility over a wide range of temperatures. With the interest to develop super high temperature alloys (next generation superalloys), a few researchers focused their attention on refractory multi-principal element alloys containing Nb, Hf, W, Cr, Zr 163 . While many new equiatomic multi-principal element alloys have been investigated in the past decade, researchers now realized the need to develop nonequiatomic multi-principal element alloys (and with additions of minor elements) to improve various properties. New subclasses in multi-principal element (MPE) alloys such as eutectic MPE alloys, MPE superalloys, TWIP-and TRIP-type MPE steels, dual-phase MPEAs, light MPE alloys, and MPE oxides have been developed in recent years[164][165][166] .Researchers attempted to predict the phase formation in multi-principal element alloys through CALPHAD approaches165  . Thermo-Calc has built a new thermodynamic database TCHEA for multi-principal element alloys. Researchers also attempted to predict it through empirical criteria based on Hume-Rothery rules, thermodynamic parameters (e.g., enthalpy of mixing Î”Hmix and entropy of mixing Î”Smix), topological parameters (e.g., Î´ parameter and mismatch entropy Î”SÏƒ), Kinetic approach (viscosity), Pettifor map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 4 .</head><label>4</label><figDesc>1 shows the workflow of predicting solidification cracking susceptibility using deep neural network (DNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Figure 4 . 1</head><label>41</label><figDesc>Figure 4.1 The workflow of predicting solidification cracking susceptibility using deep neural network (DNN).</figDesc><graphic coords="54,75.00,99.25,691.90,283.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head></head><label></label><figDesc>139,[120][121][122][123][124][125][126] . The dataset contains longitudinal varestraint test data of ferritic, austenitic, duplex, precipitation hardening (wrought / casting) stainless steels. It encompasses four types data: composition information of 16 elements in stainless steels (weight percent of C, Si, Mn, P, S, Ni, Cr, Mo, N, Nb, Co, Cu, Al, Ti, V, B), sample thickness (Th, mm) and welding parameters (welding current: I, A; welding voltage: U, V; welding velocity: Ve, mm/s), restraints (applied strain: Îµ, %) and solidification cracking susceptibility index total crack length (TCL, mm). The elements contents which were not specified in literature were manually filled as follows: Al = 0.02, N = 0.02, Mo = 0, Nb = 0, Cu = 0, V = 0, B = 0. Entries that miss five or more variables' data were not used in training and testing models. The final dataset used for machine learning contains 487 entries. Figure4.2 shows the histograms of the 22 variables in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 4 . 2 where</head><label>42</label><figDesc>Figure 4.2 Histograms of the 22 variables in the dataset (487 samples in total). Mean, minimum, maximum, std are also shown in the histograms.</figDesc><graphic coords="56,99.25,72.00,425.15,487.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.3 shows the scatter matrix of total crack length (TCL) and the other 21 variables. The Pearson correlation coefficients Rs of total crack length and the other 21 variables are also shown in the figures. If there are obvious patterns or linear relationship between them, we can easily notice them from the scatter plot. They show no apparent patterns existed in the scatter matrix. The absolute values of correlation coefficients Rs are well below 0.5 which also indicates no linear relation existed between total crack length and the other 21 variables, i.e., the relationship is nonlinear and cannot be expressed by a simple function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Figure 4 . 3</head><label>43</label><figDesc>Figure 4.3 Scatter matrix of total crack length (TCL) and other 21 variables, the values of Pearson correlation coefficients Rs are also shown in the figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head></head><label></label><figDesc>a group, only one or two variables vary. Then training data and testing data were randomly chosen in a 2 to 1 ratio. 2/3 data set (324 samples) was used for training. The remaining 1/3 data set (163 samples) was kept unseen by neural network in training to test the generalization performance (prediction accuracy on the unseen data set) of the trained neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head></head><label></label><figDesc>layers fully connected deep neural network of 21-(4-3-2)-1 and 21-(5-4-3)-1 structure, four hidden layers deep neural networks of 21-(5-4-3-3)-1 and 21-(6-5-4-3)-1 structure, and five hidden layers deep neural networks of 21-(6-5-4-3-3)-1 and 21-(5-4-3-3-3)-1 structure. The numbers in bracket represent neuron numbers of hidden layers in sequence. Initiation is vital in training deep neural network (all you need is a good initiation 173 ). A selection of initiation values for stacked autoencoder were used to generate different initiation conditions for deep neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.4 shows the schematic diagram of the process of fully connected deep neural network initiation by stacked auto-encoder (SAE). We used the four hidden layers deep neural network of 21-(6-5-4-3)-1 structure (the upper half of the diagram) as an example. C, Si, Mn, â€¢â€¢â€¢, Îµ are 21 input variables of the deep neural network. Total crack length (TCL) is the output variable of the deep neural network. W j (0) is the initial weights matrix of deep neural network's j th layer. Auto-encoder has the same input and output variables, i.e. it is a symmetry network. The neuron number of each autoencoder is equal to the neuron number of deep neural network's corresponding layer. Five auto-encoders (AE, the lower half of the diagram) of 21-(6)-21, 6-(5)-6, 5-(4)-5, 4-(3)-4, 3-(1)-3 structures were constructed in sequence. The first auto-encoder corresponds to the first hidden layer of deep neural network, and so on. Long arrows represent the directions of data transfer. The first auto-encoder took the input of deep neural network as its input and output. After training the weights and biases values of an auto-encoder's first hidden layer were transferred to the deep neural network's corresponding layer as its initial values, and the output of the previous auto-encoder's hidden layer were transferred to the next auto-encoder as its input and output. This process was carried on layer by layer and one autoencoder by one autoencoder. This unsupervised layer-wise pre-training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>Figure 4 . 4</head><label>44</label><figDesc>Figure 4.4 Schematic diagram of deep neural network initiation by stacked auto-encoder.</figDesc><graphic coords="61,99.25,99.97,415.28,312.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 4 . 5</head><label>45</label><figDesc>Figure 4.5 The regression analysis on the unseen testing dataset between experimental data (horizontal axis, T values) and neural network predictions (vertical axis, Y values). The black dotted line is Y=T. Pearson correlation coefficients Rs are also shown in the figures. (a) deep neural network's predictions, (b) shallow neural network's predictions, (c) support vector</figDesc><graphic coords="63,99.25,72.00,425.15,604.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.6(a) shows the one hidden layer neural network's training accuracy and testing accuracy as functions of the neuron number in the hidden layer. The shallow neural network's training accuracy increases from 0.73 to 0.98 and testing accuracy increases from 0.67 to 0.86 when the neuron number increases from 1 to 10. When the neuron number increases from 11 to 35, the training/testing accuracy fluctuates, and the best training and testing accuracies are achieved when the neuron number is 21 in this case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.6(b) shows the optimal training and testing accuracy of some models we tested in this study. When three deep neural networks of three to five hidden layers were trained with random initiation, they show lower testing accuracy than shallow neural networks and support vector machine. But three deep neural networks show higher testing accuracy than other models through pre-training and fine-tuning.Figure 4.6(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head>Figure 4 .</head><label>4</label><figDesc>6(b)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><head>Figure 4 . 7</head><label>47</label><figDesc>Figure 4.7 Regression analysis between total crack length predictions and brittle temperature range experimental results. (a) predictions of support vector machine, (b) predictions of shallow neural network and (c) predictions of deep neural network.</figDesc><graphic coords="66,99.25,495.17,423.74,131.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head>Figure 4 . 8</head><label>48</label><figDesc>Figure 4.8 Predicted total crack length as functions of (a) Mn, (b) Ti, (c) N, (d) Si, the compositions used in prediction are shown in the figures.</figDesc><graphic coords="67,99.25,72.00,423.73,288.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.8(c) again reproduced this metallurgical experience. Arata et al. 117 reported that Si increases solidification cracking susceptibility in fully austenitic stainless steel SUS310S, for the element Si was likely to segregate to boundaries. Our deep neural network reproduced this behavior well, as shown in Figure 4.8(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><head></head><label></label><figDesc>the normalized input vector, Y is the normalized output value, â„ ğ‘– (ğ‘˜) (k= 1, 2, â€¦, 5) is the output of the k th layer, ğ‘Š ğ‘–ğ‘—(ğ‘˜)  and ğ‘ ğ‘–(ğ‘˜)  are the weights and bias matrix of the k th layer. The structure schematic diagram of the optimal shallow and deep neural network are shown in Figure 4.9 which demonstrates the optimal 21-(6-5-4-3)-1 deep neural network has a more compact structure (the hidden layer of deep neural network is narrower than that of shallow neural network). The weights and biases values of the optimal shallow and deep neural network are shown in Figure 4.10 and Figure 4.11 respectively. The optimal 21-(6-5-4-3)-1 deep neural network has fewer parameters (210 trainable parameters) than the optimal 21-(21)-1 shallow neural network (484 trainable parameters). Neural network of fewer trainable parameters has less risk of overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.9 (a) The optimal shallow neural network's structure; (b) the optimal deep neural network's structure.</figDesc><graphic coords="69,99.25,578.11,425.05,143.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_74"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.10 The weights and biases tensors of the optimal 21-(21)-1 shallow neural network.</figDesc><graphic coords="70,99.58,245.22,424.03,127.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_75"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.11 The weights and biases tensors of the optimal 21-(6-5-4-3)-1 fully connected deep neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_76"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.12 A decision tree for predicting solidification cracking susceptibility.</figDesc><graphic coords="70,99.58,472.34,423.75,161.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_77"><head>Figure 4 .</head><label>4</label><figDesc>12 visualizes a decision tree used for predicting solidification cracking susceptibility. The decision rule (e.g., Ni â‰¤ 15.085, Strain â‰¤ 2.75), mean square error (mse), sample number, the predicted TCL value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_78"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.14 Total crack lengths of 304 and 310S stainless steel vary with strain, Cr, Ni, P, S contents vary in the specification.</figDesc><graphic coords="72,119.50,72.00,384.61,260.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_79"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.14 shows the total crack length (an indicator of solidification cracking susceptibility) variation of 304 and 310S stainless steel when elements Ni, Cr, P, S vary in the specification (compositions are shown in Table4.2). It shows that solidification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_80"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.15 The prediction of 8 stainless steels' total crack length (mm) dependence on Ni and Cr (their Ni and Cr contents are in AISI specification), compositions used in predictions are shown in Table4.3, processing parameters are the same: Th=3.18 mm, I=100 A, , U=12 V, Ve=4.23 mm/s,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_81"><head>15 (</head><label>15</label><figDesc>8 stainless steels' total crack length (an index of solidification cracking susceptibility) dependence on the main alloy elements Ni and Cr is shown in Figure 4.compositions are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_82"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.17 The prediction of stainless steels' total crack length (mm) dependence on C and N, compositions used in predictions are shown in Table 4.5. Processing parameters are the same: Th=3.18 mm, I=100 A, U=12 V, Ve=4.23 mm/s, strain=3%.</figDesc><graphic coords="77,99.25,72.00,425.15,405.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_83"><head></head><label></label><figDesc>(compositions are shown in Table 4.5). The tendency of increasing solidification cracking susceptibility with the increase of C and N is revealed in the prediction. The extent of increasing susceptibility with C and N varied among different steels due to the different alloy elements in corresponding steels. The solidification cracking susceptibility of AISI 316 is very sensitive to the C and N contents. Small difference in C and N contents can show great difference in solidification cracking behavior. But AISI301 and AISI304 are less susceptible to solidification cracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_84"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.18 The prediction of AISI 316 stainless steel's total crack length (mm) dependence on strain with C and N vary in specification, processing parameters are: Th=3.18 mm, I=100 A, U=12</figDesc><graphic coords="78,99.25,303.73,425.15,405.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_85"><head>Figure 5 .</head><label>5</label><figDesc>1 shows the workflow of predicting glass-forming ability using convolutional neural network plus 2-D representation for compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_86"><head>Figure 5 . 1</head><label>51</label><figDesc>Figure 5.1 The workflow of predicting glass-forming ability using convolutional neural network plus periodic table representation, MF, PTR and CNN denote manual features, periodic table representation and convolutional neural network, respectively.</figDesc><graphic coords="82,72.00,127.22,697.90,237.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_87"><head>Figure 5 . 2</head><label>52</label><figDesc>Figure 5.2 The statistics of elements distribution in our dataset. Many elements appear in crystalline alloys (CR), amorphous ribbon alloys (AMR), and bulk metallic glasses (BMG) simultaneously.</figDesc><graphic coords="84,99.25,72.00,425.11,216.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_88"><head>Figure 5 . 3</head><label>53</label><figDesc>Figure 5.3 The statistics of alloy systems' size in our dataset. Most alloy systems (918 out of 1134)</figDesc><graphic coords="84,152.82,527.57,317.74,210.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_89"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.3 shows the statistics of alloy systems' size in our dataset. It shows most alloy systems (918 out of 1134 alloy systems) have less than nine entries, and only two systems, i.e., Al-Ni-Zr (182 entries) and Al-La-Ni (204 entries) have over 150 entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_90"><head>Figure 5 . 4</head><label>54</label><figDesc>Figure 5.4 Elements distribution in OQMD dataset. The occurrence numbers of elements in the dataset are given under element symbols. The white squares, i.e. square with element symbol and without occurrence number, indicate these elements are not in the dataset.</figDesc><graphic coords="86,99.25,105.97,415.30,205.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_91"><head>Figure 5 . 5</head><label>55</label><figDesc>Figure 5.5 Mapping composition to 2-D representation.</figDesc><graphic coords="87,99.25,105.97,424.91,122.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_92"><head>Figure 5 . 6</head><label>56</label><figDesc>Figure 5.6 Schematic diagram for periodic table representation used in CNN3.</figDesc><graphic coords="88,138.57,412.35,346.35,206.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_93"><head>Figure 5 . 7</head><label>57</label><figDesc>Figure 5.7 Schematic diagram for randomized periodic table representation used in CNN2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_94"><head>Figure 5 . 8</head><label>58</label><figDesc>Figure 5.8 Schematic diagram for atom table representation used in CNN1.</figDesc><graphic coords="89,170.07,72.00,283.41,246.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_95"><head>where</head><label></label><figDesc>1-D vector of features (attributes/descriptors) used as input for shallow neural network include (a) statistics information of components' properties based on element properties, e.g., the maximum/minimum/average of atomic radius, Pauling electronegativity, elemental bulk modulus, elemental work function, melting point; (b) one-hot coded composition vector; (c) parameters derived from physical models and empirical criteria 141 , e.g., mixing entropy âˆ†ğ‘† ğ‘šğ‘–ğ‘¥ , mixing enthalpy âˆ†ğ» ğ‘šğ‘–ğ‘¥ , the atomic size difference âˆ†ğ‘…, the electronegativity difference âˆ†Ï‡, valence electron concentration VEC. ğ‘ ğ‘– is the atomic fraction of the i-th component; âˆ† ğ‘šğ‘–ğ‘¥ ğ´ğµ is the mixing enthalpy of alloy A-B; ğ‘Ÿ ğ‘– is the Miracle's atomic radius of the i-th component; ğœ’ ğ‘– is the electronegativity of the i-th component; ğ‘ŸÌ… is the average atoms radius of the components in the alloy; ğœ’Ì… is the average electronegativity of the components in the alloy; (ğ‘‰ğ¸ğ¶) ğ‘– is the valence electron concentration of the i-th component; VEC is the average valence electron concentration of the components in the alloy. The ğ‘Ÿ ğ‘– were taken from Ref 180 ; âˆ† ğ‘šğ‘–ğ‘¥ ğ´ğµ were taken from Ref 150 ; Pauling electronegativity, elemental bulk modulus, Elemental work function were taken from ref 181 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_96"><head>Figure 5 . 9</head><label>59</label><figDesc>Figure 5.9 Schematic diagram for our VGG-like convolutional neural network for glass-forming ability prediction.</figDesc><graphic coords="92,99.25,272.69,424.96,74.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_97"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.10 Schematic diagram for our VGG-like convolutional neural network for compound properties prediction.</figDesc><graphic coords="92,99.25,407.30,424.99,116.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_98"><head></head><label></label><figDesc>The maximum epoch was set to 2000 (loss values almost remain unchanged). All models were trained on a laptop with 2.6 GHz i7 processor and 16G memory. Training a convolutional neural network using random Initialization and AdaDelta algorithm on OQMD dataset which had over 228k entries took 12h (batch size 64, 2000 epochs in total).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_99"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.11 (a) Experimental data points of Al-Ni-Zr ternary system in our dataset and the predictions of (b) CNN3, (c) CNN2, (d) CNN1, (e) SNN4, (f) SNN3, (g) SNN2, and (h) SNN1.</figDesc><graphic coords="98,99.25,72.00,208.03,169.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_100"><head>Figure 5 .</head><label>5</label><figDesc>11 (b-h) show the prediction of CNN3, CNN2, CNN1, SNN4, SNN3, SNN2, and SNN1. CNN3 successfully predicted three amorphous composition areas, and the shapes and boundaries of these areas are satisfied when compared with the ground truth. Other models did not predict all three areas. SNN3 did not predict the crystalline area between two amorphous composition areas, i.e., the glass-forming ability of the area was overestimated. CNN2 successfully predicted two amorphous composition areas but missed the small amorphous composition areas near Ni corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_101"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.12 The prediction of Al-Ni-Zr ternary system by the re-trained models using a dataset that excluded Al-Ni, Al-Zr, Ni-Zr binary alloys and Al-Ni-Zr-containing multi-component alloys. (a)</figDesc><graphic coords="99,99.25,552.62,208.03,169.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_102"><head></head><label></label><figDesc>cross-validation on 160 ternary systems which has over 40 entries in our dataset. In LOSO cross-validation for a ternary system A-B-C, entries of A-B, A-C, B-C binary alloys and A-B-C ternary alloys were held out as testing dataset. Models were trained with the remaining dataset. The average testing accuracies of SNN4, CNN2 and CNN3 under LOSO cross-validation are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_103"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.13 Experimental results and the predictions of S-bearing alloys TixZr75-xNi12Cu5S8, TixZr75-xNi17S8, TixZr75-xCu17S8 (x=0-35).</figDesc><graphic coords="103,99.25,72.00,425.15,261.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_104"><head>( 3 )</head><label>3</label><figDesc>Validation using RE6Fe72B22 and specially designed binary alloys In this part, we used two groups of easily confused alloys to validate the models' generalization ability. The first group includes 13 RE6Fe72B22 alloys (RE: Sc, Y, La, Ce, Pr, Nd, Sm, Eu, Gd, Tb, Dy, Ho, Er) 187 . Rare earth elements are easily confused elements for their close physical and chemical properties. Experimental results show the simple substitution of similar elements causes the glass-forming ability variations of RE6Fe72B22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_105"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.14 shows our convolutional neural network regression accuracy (defined as Pearson correlation coefficient between prediction values and target values) on OQMD testing dataset (20% the dataset, about 45k entries). Based on compositions only, the convolutional neural network can achieve high accuracy of 0.997 in predicting compound forming energy (Ef) and 0.993 in compound specific volume (V). The networks'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_106"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.14 The regression analysis between ground truth (horizontal axis, T values) and convolutional neural networks' predictions (vertical axis, Y values) of (a) compounds' forming energy Ef (b) compounds' specific volume. Testing accuracy R is also shown in the figure.</figDesc><graphic coords="106,99.25,72.00,415.30,205.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_107"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.15 The electron configuration, physical and chemical properties of elements show periodic variation in periodic table, e.g. atomic radius decreases and Pauling electronegativity increases from left to right within a period; atomic radius increases and electronegativity</figDesc><graphic coords="106,99.25,417.26,425.08,278.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_108"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.17 Illustration of the 192-dimensional feature vectors of 108 elements extracted by CNN2. The values of features are represented by color (see the color bar to the right).</figDesc><graphic coords="108,99.25,358.16,425.08,226.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_109"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.18 Illustration of the 128-dimensional feature vectors of 108 elements extracted by CNN1. The values of features are represented by color (see the color bar to the right).</figDesc><graphic coords="109,125.60,455.61,372.43,282.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_110"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.19 Features analysis of the glass-forming ability prediction model after principal component analysis (PCA). Projection of the feature vectors of 108 elements onto the plane spanned by the first and second principal axes. The percentage represents the ratio of the variance on the principal axis direction. Elements are colored according to their groups. (a) periodic table representation; (b) randomized periodic table representation. The superscript 1-18 on element symbol represents the element's group number; superscripts 19 and 20 represent lanthanide and actinides, respectively.</figDesc><graphic coords="110,125.02,72.00,373.57,283.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_111"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.19(a) illustrates the glass-forming ability knowledge of 108 elements in periodic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_112"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.19 (a) can be used for amorphous alloys design. Element substitution is one of the most effective ways to improve the glass-forming ability of amorphous alloys.However, selecting suitable substituting elements is very challenging. Researchers used "column substitution" (substitution with elements of the same group) and "row substitution" (substitution with neighboring elements in the same period) to simplify the process. Neighboring elements in Figure5.19 (a) have close features, so they are good candidates for element substitution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_113"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.20 illustrates the knowledge of 80 elements in periodic table (actinides, noble gases, elements of 7 th period are excluded for clarity) generated by feature extractors trained on OQMD dataset. Elements of a group (the group number of each element is shown to the top right of the element symbol) cluster in a small region (different regions / groups marked with different colors in the figure). There are apparent periodic trends in the distribution of these groups: group 1 (alkali metal) to group 17 (halogens) distribute along a circle (not shown in figure) one by one; elements in a group distribute along radial direction according to ascending atomic number; lanthanides distribute in the center of the circle and close to group 3 (elements Sc and Y which are similar to lanthanides in many aspects); all transition metallic elements situate at the up left part; all nonmetallic elements locate at the bottom part. The spatial distribution of elements in Figure 5.20 resemble a 3-D periodic table reflecting some features of elements. These patterns indicate feature extractors have successfully learned knowledge of elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_114"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.20 Visualization of the first three principal features (after principal component analysis) of 80 pure elements. The percentage represents the ratio of the variance on the principal axis direction. Elements are colored according to their elemental groups. The superscript 1-17 on the element symbol represents the element's group number; superscript * represents lanthanide.</figDesc><graphic coords="112,99.25,72.00,415.30,362.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_115"><head>Figure 6 .</head><label>6</label><figDesc>1 shows workflow for transfer learning from predicting glass-forming ability to predicting phase formation of multi-principal element alloys and transfer learning from predicting inorganic compound's forming energy Ef and specific volume V to predicting inorganic compound's phase prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_116"><head>Figure 6 . 1 6. 1 6 . 1 . 1</head><label>611611</label><figDesc>Figure 6.1 The workflow of transfer learning (a) GFA to MPEA; (b) transfer from OQMD to phase prototype. GFA, AM, CR, MEPA, Conv, OQMD, CNN, SNN, RF denotes glass-forming ability, amorphous, crystalline, multi-principal element alloys, convolutional operation, fully connected layer, open quantum materials database, convolutional neural network, shallow neural network,</figDesc><graphic coords="114,99.25,463.53,425.10,203.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_117"><head>Figure 6 . 2</head><label>62</label><figDesc>Figure 6.2 Statistics of the 355 multi-principal element alloys in the dataset. (a) The numbers of binary to nonary multi-principal element alloys and the proportions of different phases. (b) The occurrence numbers of elements in the dataset are given under periodic table background. The blank squares, e.g. squares for noble gases, signify the elements not in the dataset.</figDesc><graphic coords="116,99.25,72.00,425.15,211.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_118"><head>Figure 6 . 5</head><label>65</label><figDesc>Figure 6.5 ROC curves of the transfer learning models for predicting multi-principal element alloy</figDesc><graphic coords="122,149.82,413.65,324.00,324.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_119"><head>6. 3 . 2 Figure 6 . 6</head><label>3266</label><figDesc>Figure 6.6 shows the four models' classification accuracy versus test ratio (the ratio of test dataset to the entire dataset) on phase prototypes dataset. All four models are built on chemistry compositions only. However, the accuracy advantage of models based on transfer learning over the model based on simple composition vector is obvious. When testing ratio is 0.1 (i.e. 90% of the dataset are for training and 10% of the dataset are hold out for testing), transfer learning model based on Vector Ef&amp;V can reach accuracy of 0.9, while non-transfer-learning model based on Vector comp only got accuracy of 0.54.The accuracy of transfer learning model is relatively insensitive to the test ratio. When the test ratio increases from 0.1 to 0.5 (i.e. the ratio of training dataset to entire dataset decreases from 0.9 to 0.5), the accuracy only decreases from 0.9 to 0.85. The insensitivity of transfer learning models to the size of training dataset indicates it has good generalization and suitable for small datasets. The accuracy differences between 3 transfer learning models are small (about 0.01 -0.03). The model based on Vector Ef&amp;V shows the highest accuracy because it can access features from both feature extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_120"><head>Figure 6 . 6</head><label>66</label><figDesc>Figure 6.6 Accuracy vs. testing ratio of three transfer learning models using feature vectors</figDesc><graphic coords="125,118.93,419.59,385.80,261.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_121"><head>Figure 6 . 7</head><label>67</label><figDesc>Figure 6.7 The multi-principal element alloys' first four principal features generated by glassforming ability model. Alloys are colored according to their phases. The percentage represents the ratio of the variance on the principal axis direction.</figDesc><graphic coords="127,157.32,365.30,308.40,251.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_122"><head>3 -</head><label>3</label><figDesc>D space using the first three principal components of their feature vectors. The four common phase prototypes (shown in Figure 6.8) which have hundreds of entries in our dataset (they are shown in big symbols in the word cloud, see Error! Reference source n ot found.) are 'MgCu2, cF24, 227', 'Ca2Nb2O7, cF88, 227', 'Cu, cF4, 225', and 'ZnS, cF8, 216'. Many thousands of inorganic substances in total contained in four different prototype clusters to four different regions (marked with different colors). They can easily be discriminated only by the first three principal components, which shows that feature extractors are highly effective. The 15 uncommon phase prototypes (shown in Figure 6.9) which have limited entries (20-40 entries) in our dataset (they are shown in very small symbols in the word cloud see Error! Reference source not found.) are 'CaB6, cP7, 221', 'YbFe2Al10, oS52, 63', ' Pr3WCl3O6, hP26, 176', 'Y4PdGa12, cI34, 229', 'YCo5P3, oP36, 62', 'K3Nb8O21, hP64, 193', 'Y6RuI10, aP17, 2', 'KGdNb6Cl18, hR81, 148', 'KAsF6, hR24, 148', 'Cs3Tl2Cl9, hR84, 167', 'Ba2Cu4YO8, oS30, 65', 'Hf9Mo4B, hP28, 194', 'Er3CrB7, oS44, 63', 'CsMn2P6O18, mS56, 12', 'U3Ni4Si4, oI22, 71'. Substances of different prototype cluster to different regions (marked with different colors). The first three principal components can be easily used in distinguishing the 15 uncommon phase prototypes. It further validates the effectiveness of the features yielded by the transferable feature extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_123"><head>Figure 6 . 8</head><label>68</label><figDesc>Figure 6.8 Visualization of the first three principal features (after principal component analysis) of substances belonging to four common phase prototypes in our dataset. Inorganic substances are colored according to their phase prototypes. The percentage represents the ratio of the variance on the principal axis direction.</figDesc><graphic coords="129,104.75,448.49,415.30,272.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_124"><head>Figure 6 . 9</head><label>69</label><figDesc>Figure 6.9 Visualization of the first three principal features (after principal component analysis) of substances belonging to 15 uncommon phase prototypes in our phase prototypes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_125"><head></head><label></label><figDesc>an effective method to inject knowledge into a machine learning model. To further improve the generalization of a deep learning model and to reduce the dependence on the dataset volume, we can add more domain expertise to data representation. How to effectively inject knowledge to 2-D data representation is an open question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="71,99.25,143.91,425.15,177.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="115,164.00,461.39,295.65,297.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="117,99.25,143.92,415.30,205.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="117,99.25,457.54,415.30,274.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 .</head><label>4</label><figDesc>3 Compositions (wt%) of stainless steels used in prediction (varied Ni and Cr). ........................... 55</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>4 Compositions (wt%) of stainless steels used in prediction (varied P and S). .............................. 57</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>5 Compositions (wt%) of stainless steels used in prediction (varied C and N). ............................. 58</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>1 The statistics of labels in our dataset. CR, AMR, and BMG denote crystalline, amorphous ribbon, and bulk metallic glasses, respectively. .............................................................................................. 65</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>2 Matrix for transforming original labels of ternary classification to new labels of binary classification. CR and AM denote crystalline state, amorphous state, respectively. ......................... 66</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>3 The list of manual features used in building machine learning models. Features used in SNN1 and SNN3 are denoted with *. Other features combinations were also tested at the beginning but did not show noticeable improvement in accuracy, so they were not used in final model. .......................... 71</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>4 Hyperparameter details of CNN1 (atom table representation), CNN2 (randomized periodic table representation) and CNN3 (periodic table representation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>2.4.2 Convolutional neural network</head><label></label><figDesc></figDesc><table><row><cell>proved that deep neural network uses fewer parameters (neurons) than</cell></row><row><cell>shallow neural network to fit a task (dataset). Figure 2.7 schematically shows the</cell></row><row><cell>difference. Researchers have been exploring the effective ways to train deep neural</cell></row><row><cell>network which have thousands of layers like humans' long connected neural system</cell></row><row><cell>since neural network was firstly proposed 3,23 . Training a deep neural network like training</cell></row><row><cell>a shallow neural network generally usually does not work: the performance of deep</cell></row><row><cell>neural network always worse than that of shallow neural network 51 . Researchers</cell></row><row><cell>believed that gradient vanishing with increasing depth of network and traps of poor local</cell></row><row><cell>minimums were the main difficulties in training deep neural network 4,52,53 . Hinton</cell></row><row><cell>proposed restricted Boltzmann machine (RBM) and deep belief network (DBN) in 2006.</cell></row><row><cell>Deep neural network then can be successfully trained through a so-called greedy layer-</cell></row><row><cell>wise pre-training 53 . Later, Bengio proposed stacked auto-encoder (SAE) as an alternative</cell></row><row><cell>to restricted Boltzmann machine in pre-training 54 . Since the success of deep belief</cell></row><row><cell>network, deep neural network is thriving. Deep neural networks, especially the</cell></row><row><cell>convolution neural network, accomplished unprecedented successes in image</cell></row><row><cell>recognition. Deep learning (deep neural network) has become the mainstream in</cell></row><row><cell>machine learning now.</cell></row><row><cell>Convolutional neural network (CNN), a special type of neural network, is widely</cell></row><row><cell>employed in processing images 55,56 , e.g. images classifications, object detection, image</cell></row><row><cell>semantic segmentation. A Convolutional neural network 3 usually has a series of</cell></row><row><cell>convolution layers with filters (kernels), pooling layers, ReLU layers, fully connected</cell></row><row><cell>layers (FC). Softmax function layer is usually used as output layer in classification.</cell></row><row><cell>Convolution kernels can realize complicate image operations such as image edge</cell></row><row><cell>detection, sharpen and blur. Stride is the number of pixels we move the filters at a time</cell></row><row><cell>over the input matrix. Sometimes filter does not perfectly fit the input image. We</cell></row><row><cell>commonly pad the picture with zeros (zero-padding). ReLU represents rectified linear</cell></row><row><cell>unit for a nonlinear operation. Its mathematic expression is Æ’(x) = max(0, x). Pooling</cell></row><row><cell>(subsampling or downsampling) layers (max pooling, average pooling, sum pooling)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">.1 Challenges in machine learning for materials research.</cell></row><row><cell cols="2">Elements Aspects</cell><cell></cell><cell cols="2">Typical ML applications (CV</cell><cell cols="2">Material science applications</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and NLP)</cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell>Dataset size</cell><cell></cell><cell cols="2">Big dataset (usually &gt;10 4</cell><cell>Small</cell><cell>dataset</cell><cell>(usually</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">examples, up to 10 8 are</cell><cell>between</cell><cell>10 1 -10 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>available)</cell><cell></cell><cell cols="2">examples, &gt;10 4 are rare)</cell></row><row><cell></cell><cell>Data</cell><cell>X</cell><cell>Dense, balanced</cell><cell></cell><cell cols="2">Sparse, correlated, clustered,</cell></row><row><cell></cell><cell>distribution</cell><cell></cell><cell></cell><cell></cell><cell cols="2">unbalanced X</cell></row><row><cell></cell><cell>Label</cell><cell>Y</cell><cell cols="2">Balanced; sample bias often</cell><cell cols="2">Unbalanced labels Y; biased</cell></row><row><cell></cell><cell>distribution</cell><cell></cell><cell>present</cell><cell></cell><cell cols="2">labels (bad/failure results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">unreported)</cell></row><row><cell></cell><cell cols="2">Error/noise in</cell><cell>Not important</cell><cell></cell><cell cols="2">Always exist, essential for</cell></row><row><cell></cell><cell cols="2">data X and</cell><cell></cell><cell></cell><cell cols="2">model performance</cell></row><row><cell></cell><cell>label Y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Data type</cell><cell></cell><cell>Often standardized</cell><cell></cell><cell>Rarely</cell><cell>standardized,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">heterogeneous</cell></row><row><cell></cell><cell cols="3">Data augment Applicable</cell><cell></cell><cell cols="2">Usually not applicable</cell></row><row><cell>Features</cell><cell>Descriptors,</cell><cell></cell><cell cols="2">Standard methods are</cell><cell cols="2">Manual feature engineering</cell></row><row><cell></cell><cell>data</cell><cell></cell><cell cols="2">available, e.g. SIFT; rely on</cell><cell>requires</cell><cell>deep</cell><cell>domain</cell></row><row><cell></cell><cell cols="2">representation</cell><cell cols="2">data to learn patterns; can</cell><cell cols="2">knowledge</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">often be optimized by</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>algorithms</cell><cell></cell><cell></cell></row><row><cell>Tasks</cell><cell>Aims</cell><cell></cell><cell cols="2">Accurately pattern-match</cell><cell cols="2">Need to predict unusual or</cell></row><row><cell></cell><cell></cell><cell></cell><cell>common</cell><cell>cases,</cell><cell cols="2">"extreme"</cell><cell>materials,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Interpolation</cell><cell></cell><cell cols="2">extrapolation, optimization</cell></row><row><cell></cell><cell>Prediction</cell><cell></cell><cell cols="4">Rough results are accepted Need high prediction accuracy</cell></row><row><cell></cell><cell>accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Prediction</cell><cell></cell><cell>Usually unimportant</cell><cell></cell><cell cols="2">Uncertainty</cell><cell>and</cell><cell>error</cell></row><row><cell></cell><cell>error</cell><cell></cell><cell></cell><cell></cell><cell cols="2">estimation are desired</cell></row><row><cell></cell><cell>estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell>Usually unimportant</cell><cell></cell><cell cols="2">Interpretability is desired if</cell></row><row><cell></cell><cell cols="2">interpretability</cell><cell></cell><cell></cell><cell>possible</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Facing multiple variables nonlinear problem, data-based and data-driven statistical learning methods, e.g., neural network (NN), increasingly become an alternative to physically based analytical and numerical methods. Based on reliable experimental data, neural network is able to depict alloy solidification cracking susceptibility as a function of composition and processing parameters in high-dimensional space, like that was done by Ichikawa 140 . Ichikawa et al. employed a neural network to predict solidification cracking of welds in low-alloy steels. They assembled a small dataset of 154 samples. The input variables are chemistry (weight percent of C, Si, Mn, P, S, Ni, Cr, Mo) and welding parameters (the welding current in amperes; the welding voltage in voltages; the welding travel speed in cm/min; the weld preparation groove angle in degrees; the</figDesc><table /><note><p>preheat temperature in centigrade). The output variable is solidification cracking sensitivity (0 represents not cracking and 1 represents cracking). The neural network model successfully reproduced the nonlinear relationship of solidification cracking sensitivity dependence on chemistry and welding parameters.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>[141][142][143] , magnetic144  , catalytic145  , physical properties 146 and unique structure of lacking long-range order 147 , amorphous alloys and metallic glasses have been widely studied and explored as a promising new material since they were first The glass-forming ability (GFA) of an alloy, i.e., the critical cooling rate below which the alloy melt undergoes nucleation and growth and forms crystal (CR), is a fundamental problem in developing new amorphous alloys. In the past five decades, many empirical criteria and physical models about glass-forming ability have been postulated.</figDesc><table><row><cell>Dimensionless parameters based on liquidus temperature TL, glass transformation</cell></row><row><cell>temperature Tg and crystallization temperature Tx, e.g., Turnbull's Tg/TL, Lu's Tx/(Tg +</cell></row><row><cell>TL) 149 were proposed to estimate the glass-forming ability of amorphous alloys. But these</cell></row><row><cell>parameters can only be obtained after the successful preparation of amorphous alloys</cell></row><row><cell>and cannot be used to predict new amorphous alloys. Inoue 150 proposed the famous</cell></row><row><cell>three basic empirical rules of bulk metallic glass formation (high glass-forming ability):</cell></row><row><cell>discovered in the 1960s 148 . Two methods are commonly used to prepare amorphous</cell></row><row><cell>alloys, i.e. single wheel melt-spun (cooling rate can be as high as 10 6 -10 5 K/s) and copper Ward 58 et al. predicted the glass-forming ability of ternary alloys using random forest</cell></row><row><cell>mold casting (cooling rate in the range of 10 2 -10 -1 K/s). Melt-spun is a rapid solidification classification models with 145 'general purpose' attributes derived from the periodic</cell></row></table><note><p><p><p><p>process and its product is amorphous ribbon of about 20 -100Î¼m in thickness and about 1mm to several mm in width. Copper mold casting is commonly used in preparing bulk metallic glasses with each dimension above 1mm. multicomponent alloys containing three or more elements, large negative mixing enthalpy, large atomic size difference. However, Louzguine-Luzgin's research 151 shown that many alloys satisfy all three criteria but still cannot form an amorphous ribbon with rapid solidification process. Miracle 152,153 linked the glass-forming ability with geometric packing and put forward the efficient cluster packing model. Greer formulated the confusion model based on the complexity of the solidification path</p>154,155  </p>. Other works related glass-forming ability to deep eutectic area 156 , Pauling electronegativity 157 . Due to the vague physical mechanisms of glass-forming ability, developing new amorphous alloys is still mainly through time-consuming and costly trials and errors.</p>Sun 158 et al. attempted to predict the glass-forming ability of binary alloys using support vector machine classification model. Manual features, e.g., atomic weights, the mixing enthalpy, atomic radii, liquidus temperatures, the fictive temperatures, the difference in liquidus temperature were used as input descriptors (features, attributes). A dataset containing 31 binary alloy systems was assembled. Their results indicated that the difference between liquidus temperature and the fictive temperature is an important feature for predicting glass-forming ability of binary alloys. table. A dataset of about 6000 entries was collected from handbooks 159 . The model created could reach an accuracy of 90% under 10-fold cross-validation. Based on the work, Ward 57 et al. assembled a larger dataset of 8000 entries from literature. This new dataset includes bulk metallic glass, amorphous ribbon, and crystalline data. The 145 'general purpose' attributes plus cluster packing efficiency attributes, nearest special clusters, mean packing efficiency, and proximity to crystalline compound attributes. 201</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>of deep neural network and pre-training to predicting solidification cracking susceptibility with small dataset</head><label></label><figDesc>By selecting the threshold from 150 to 10, the classification performance metrics (accuracy, precision, and recall) vary as follows: accuracy ranged from 0.97 to 0.85; average precision ranged from 0.86 to 0.79, while average recall ranged from 0.73 to 0.54 for threshold from 150 to 10, respectively.Accurately predicting solidification cracking susceptibility of alloys is challenging. Though the factors that affect solidification cracking are very clear (i.e., descriptor is not a problem), the function is nonlinear, and experimental data are heterogeneous and insufficient. How to build machine learning models of good performance and interpretability based on our existing small dataset is the subject of this chapter.</figDesc><table /><note><p>Graser 172 et al. assembled phase prototype dataset from Pearson's Crystal database and used random forest to classify them. To take all data in model, labels of entries number below a threshold were relabeled as "Other". 4 Application</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 4 .</head><label>4</label><figDesc>1 The best testing accuracies of five models used in prediction.</figDesc><table><row><cell>Model</cell><cell>Decision</cell><cell>Random</cell><cell>Support vector</cell><cell>Shallow neural</cell><cell>Deep neural</cell></row><row><cell></cell><cell>tree</cell><cell>forest</cell><cell>machine</cell><cell>network</cell><cell>network</cell></row><row><cell cols="2">Testing accuracy 0.75</cell><cell>0.88</cell><cell>0.89</cell><cell>0.89</cell><cell>0.93</cell></row></table><note><p>After training, Pearson correlation coefficients Rs of the target values and decision tree / random forest / support vector machine / shallow neural network / deep neural network prediction values were calculated as the index of training/testing accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Code C</cell><cell>Si</cell><cell>Mn P</cell><cell>S</cell><cell>Cr</cell><cell>Ni</cell><cell>N</cell><cell>Al</cell><cell>Th</cell><cell>I</cell><cell>U Ve</cell></row><row><cell>304</cell><cell cols="3">0.06 0.5 1.5 0.005-</cell><cell>0.005-</cell><cell>18-</cell><cell>8-</cell><cell cols="5">0.02 0.02 3.18 100 12 4.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.03</cell><cell>0.03</cell><cell>20</cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">310S 0.01 0.5 1.5 0.005-</cell><cell>0.005-</cell><cell>24-</cell><cell>19-</cell><cell cols="5">0.02 0.02 3.18 100 12 4.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.03</cell><cell>0.03</cell><cell>26</cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>2 </p>The specifications (wt%) of 304 and 310S stainless steel and the testing parameters used in prediction.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>.3, processing parameters are the same: Th=3.18 mm, I=100 A, , U=12 V, Ve=4.23 mm/s,</cell></row><row><cell>strain=3%.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 4 .</head><label>4</label><figDesc>3 Compositions (wt%) of stainless steels used in prediction (varied Ni and Cr).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 4</head><label>4</label><figDesc>Figure 4.16 The prediction of 8 stainless steels' total crack length (mm) dependence on P and S, compositions used in predictions are shown in Table4.4, processing parameters are the same:</figDesc><table><row><cell>.3). The predictions are in good accordance with</cell></row><row><cell>metallurgical experience. Fully austenitic stainless steels like AISI 316 is more susceptible</cell></row><row><cell>to solidification cracking than stainless steels contain a certain amount of ferrite like AISI</cell></row><row><cell>304. Element C, which tends to segregate to grains boundaries, increase cracking</cell></row><row><cell>susceptibility (see the contour difference between AISI 316L and AISI 316). Machine</cell></row><row><cell>learning regression has transformed scattered data points into an expressive high-</cell></row><row><cell>dimensional map, and those contours are only some slices of it. The tendency of</cell></row><row><cell>increasing solidification cracking susceptibility with the increase of Ni is illustrated in the</cell></row><row><cell>figure. Ferrite can accept more impurity elements (like S and P) than austenite. Irregular</cell></row><row><cell>ferrite/austenite grain boundary is not in favor of the propagation of cracking cracks.</cell></row><row><cell>Thus, a High ratio of Cr to Ni (or Cr equivalent to Ni equivalent) is good for solidification</cell></row></table><note><p><p>cracking resistance by forming a certain amount of ferrite like that in AISI 301 and 304.</p>(3) Solidification cracking susceptibility dependence on P and S</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 4 .</head><label>4</label><figDesc>5 Compositions (wt%) of stainless steels used in prediction (varied C and N).</figDesc><table><row><cell cols="2">Code C</cell><cell>Si</cell><cell>Mn P</cell><cell>S</cell><cell>Cr</cell><cell>Ni</cell><cell cols="2">Mo N</cell><cell>Nb Cu Al Ti</cell></row><row><cell>301</cell><cell>0.001-</cell><cell>1.0</cell><cell cols="5">2.0 0.02 0.02 17.0 7.0 -</cell><cell>0.001-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row><row><cell>304</cell><cell>0.001-</cell><cell>1.0</cell><cell cols="5">2.0 0.02 0.02 19.0 9.5 -</cell><cell>0.001-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row><row><cell>316</cell><cell>0.001-</cell><cell cols="7">0.75 2.0 0.02 0.02 17.0 12.0 2.0 0.001-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row><row><cell>321</cell><cell>0.001-</cell><cell cols="6">0.75 2.0 0.02 0.02 18.0 9.5 -</cell><cell>0.001-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc><ref type="bibr" target="#b135">57,</ref><ref type="bibr" target="#b136">58</ref> et al. show machine learning is a promising method for predicting glass-forming ability and developing new amorphous alloys. But the conventional machine learning models they used work well only when data is sufficient and features are well-designed. However, both data and descriptors are deficient in the materials field. Successful manual feature engineering, i.e., transforming raw data into direct input for models, is very challenging and needs a deep understanding of the physical mechanisms of the problems. In this scenario, taking advantage of convolutional neural networks' automatic features extraction ability and adding domain expertise into data representations are better solutions3,56 .    </figDesc><table><row><cell>method, we compared it with convolutional neural networks using representations</cell></row><row><cell>without the periodic table information and conventional shallow neural network (SNN)</cell></row><row><cell>using manual feature engineering. The features (i.e. the intermediate results of</cell></row><row><cell>convolutional neural networks) extracted were also visualized to prove the domain</cell></row><row><cell>expertise was presented in the extracted features.</cell></row></table><note><p><p><p><p><p>Recently,</p>Zheng et </p>al. proposed a periodic table representation (PTR) for alloy compositions and combined it with convolutional neural networks (CNN) to predict compound formation energy 175 . Zeng et al. proposed an atom table convolutional neural networks (ATCNN) for predicting superconducting critical temperature</p>176  </p>. Both methods mapped compositions to 2-D images and exploited convolutional neural networks to extract features and make regression automatically. Their results showed the advantage of high prediction accuracy and end-to-end learning. This chapter reported our attempt to predict glass-forming ability using convolutional neural networks with a modified periodic table representation. We used a medium dataset, however for most alloy systems in the dataset it is still a small dataset task. To show the advantage of this</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>BMG</cell><cell cols="2">AMR CR</cell><cell>total</cell></row><row><cell>Pure</cell><cell>0</cell><cell>0</cell><cell>84</cell><cell>84</cell></row><row><cell>Binary</cell><cell>17</cell><cell cols="3">1108 2607 3732</cell></row><row><cell>Ternary</cell><cell>279</cell><cell cols="3">3520 1369 5168</cell></row><row><cell cols="2">quaternary &amp; other 650</cell><cell>2</cell><cell>804</cell><cell>1456</cell></row><row><cell>sum</cell><cell>946</cell><cell cols="3">4630 4864 10440</cell></row></table><note><p><p>1 </p>The statistics of labels in our dataset. CR, AMR, and BMG denote crystalline, amorphous ribbon, and bulk metallic glasses, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 5 .</head><label>5</label><figDesc>2. The size of our original dataset is 10440, and the size of our final dataset</figDesc><table><row><cell>(after transformation) is 16250 (946*2 + 4630 + 4864*2).</cell></row></table><note><p>The ideal parameters to describe glass-forming ability of an alloy are critical cooling rate and critical thickness. Due to the incompleteness of them in our dataset, we simplified the regression problem as a classification problem. Considering that we should not arbitrarily declare that all amorphous ribbon alloys in our dataset cannot form bulk metallic glasses, we did not simply treat it as a problem of (CR / AMR / BMG) ternary classification. We added a process parameter (0 represents rapid solidification melt-spun, and 100 represent copper mold casting of normal cooling rate) into this problem to convert the ternary classification problem into a problem of (AM / CR) binary classification. AM represents forming an amorphous state, and CR represents forming a crystalline state, see the matrix for transforming the original labels to the new labels in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 5 .</head><label>5</label><figDesc>2 Matrix for transforming original labels of ternary classification to new labels of binary classification. CR and AM denote crystalline state, amorphous state, respectively.</figDesc><table><row><cell></cell><cell cols="2">Melt-spun Copper mold casting</cell></row><row><cell>CR</cell><cell>CR</cell><cell>CR</cell></row><row><cell cols="2">BMG AM</cell><cell>AM</cell></row><row><cell cols="2">AMR AM</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>the 1 st pixel/square in the 1 st row is used to store the atomic percentage of element Hydrogen in an alloy. The 54 grey squares are the unused area in the periodic table. The alloy composition (in atomic</figDesc><table><row><cell>percentage) is mapped to the corresponding blue squares, and the preparation process</cell></row><row><cell>(0 represents melt-spun, and 100 represents copper mold casting) is mapped to a grey</cell></row><row><cell>square (we arbitrarily chose the 9 th pixel/square in the first row in this work). The rest</cell></row><row><cell>pixels/squares are set to 0.</cell></row><row><cell>The randomized periodical table representation used in model CNN2 is almost the same</cell></row></table><note><p>table (see Figure 5.6) and mapped processing parameters to the unused area in the periodic table; on the contrary, Zheng's periodical table representation 175 did not contain rare earth elements and processing parameters. In order to validate the benefit of merging periodic table knowledge into data representation, two other mapping methods without periodic table structure were also used: i.e., randomized periodic table representation (see Figure 5.7) and atom table representation 176 (see Figure 5.8). Periodic table representations (used in model CNN3) mimic digital images. Alloy composition and preparation processes are mapped to a 2-dimensional image of 9 pixels by 18 pixels (162 pixels in total). Each square represents a pixel. The 108 blue squares correspond to 108 elements in the periodic table, e.g., with periodical table representation except 108 elements were randomly placed in the periodical table area (see Figure 5.7). The atom table representation used in model CNN1 are square images of 11*11 pixels, elements are placed in an atom table from left to</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">3 The list of manual features used in building machine learning models. Features used in</cell></row><row><cell cols="6">SNN1 and SNN3 are denoted with *. Other features combinations were also tested at the</cell></row><row><cell cols="6">beginning but did not show noticeable improvement in accuracy, so they were not used in final</cell></row><row><cell>model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixing entropy</cell><cell cols="2">(Max / Min / Average)</cell><cell>(Max / Min / Average)</cell><cell cols="2">(Max / Min /</cell></row><row><cell>âˆ†ğ‘† ğ‘šğ‘–ğ‘¥ *</cell><cell>Miracle</cell><cell>atomic</cell><cell>Pauling</cell><cell cols="2">Average) elemental</cell></row><row><cell></cell><cell>Radius*</cell><cell></cell><cell>electronegativity*</cell><cell cols="2">bulk modulus*</cell></row><row><cell>(Max / Min /</cell><cell cols="2">(Max / Min / Average)</cell><cell>(Max / Min / Average)</cell><cell cols="2">(Max / Min /</cell></row><row><cell>Average)</cell><cell cols="2">melting temperature</cell><cell>boiling temperature</cell><cell>Average)</cell><cell>atomic</cell></row><row><cell>elemental work</cell><cell></cell><cell></cell><cell></cell><cell>number</cell></row><row><cell>function*</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Max / Min /</cell><cell cols="2">(Max / Min / Average)</cell><cell>(Max / Min / Average)</cell><cell cols="2">(Max / Min /</cell></row><row><cell>Average) atomic</cell><cell>covalent radius</cell><cell></cell><cell>ionic radius</cell><cell cols="2">Average) elemental</cell></row><row><cell>weight</cell><cell></cell><cell></cell><cell></cell><cell cols="2">concentration</cell></row><row><cell>Mixing enthalpy</cell><cell cols="2">Atomic size difference</cell><cell>Electronegativity</cell><cell>Valence</cell><cell>electron</cell></row><row><cell>âˆ†ğ» ğ‘šğ‘–ğ‘¥</cell><cell>âˆ†ğ‘…</cell><cell></cell><cell>difference âˆ†Ï‡</cell><cell cols="2">concentration VEC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">4 Hyperparameter details of CNN1 (atom table representation), CNN2 (randomized</cell></row><row><cell cols="7">periodic table representation) and CNN3 (periodic table representation), CNN2 and CNN3 are</cell></row><row><cell cols="4">assigned with the same hyperparameters.</cell><cell></cell><cell></cell></row><row><cell cols="2">Order Layer (type)</cell><cell>Kernel</cell><cell>Stride</cell><cell cols="2">CNN1 Param #</cell><cell>CNN2 &amp; CNN3 Param #</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(atom</cell><cell>table</cell><cell>(periodic</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">representation of</cell><cell>representation of 9*18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>11*11 pixels)</cell><cell></cell><cell>pixels)</cell></row><row><cell>1</cell><cell>Conv2d</cell><cell>8*3*3</cell><cell>1*1</cell><cell>80</cell><cell></cell><cell>80</cell></row><row><cell>2</cell><cell cols="2">MaxPooling2D 2*2</cell><cell>2*2</cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell>3</cell><cell>Conv2d</cell><cell cols="2">16*3*3 1*1</cell><cell>1168</cell><cell></cell><cell>1168</cell></row><row><cell>4</cell><cell cols="2">MaxPooling2D 2*2</cell><cell>2*2</cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell>5</cell><cell>Conv2d</cell><cell cols="2">32*3*3 1*1</cell><cell>4640</cell><cell></cell><cell>4640</cell></row><row><cell>6</cell><cell cols="2">MaxPooling2D 2*2</cell><cell>1*1</cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell>7</cell><cell>Flatten</cell><cell>-</cell><cell>-</cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell>8</cell><cell>Dense</cell><cell>-</cell><cell>-</cell><cell>258</cell><cell></cell><cell>386</cell></row><row><cell cols="2">Total params</cell><cell></cell><cell></cell><cell>6146</cell><cell></cell><cell>6274</cell></row><row><cell cols="2">Trainable params</cell><cell></cell><cell></cell><cell>6146</cell><cell></cell><cell>6274</cell></row><row><cell cols="2">Non-trainable params</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 5 .</head><label>5</label><figDesc>5 The summary of our VGG-like convolutional neural network used in the prediction of compounds' forming energy Ef and specific volume V.</figDesc><table><row><cell>Layer (Type)</cell><cell>Output Shape</cell><cell>Param #</cell></row><row><cell>conv2d_1 (Conv2D)</cell><cell>(None, 8, 9, 18)</cell><cell>80</cell></row><row><cell>conv2d_2 (Conv2D)</cell><cell>(None, 8, 9, 18)</cell><cell>584</cell></row><row><cell>max_pooling2d_1 (MaxPooling2D)</cell><cell>(None, 8, 5, 9)</cell><cell>0</cell></row><row><cell>conv2d_3 (Conv2D)</cell><cell>(None, 16, 5, 9)</cell><cell>1168</cell></row><row><cell>conv2d_4 (Conv2D)</cell><cell>(None, 16, 5, 9)</cell><cell>2320</cell></row><row><cell>max_pooling2d_2 (MaxPooling2D)</cell><cell>(None, 16, 3, 5)</cell><cell>0</cell></row><row><cell>conv2d_5 (Conv2D)</cell><cell>(None, 32, 3, 5)</cell><cell>4640</cell></row><row><cell>conv2d_6 (Conv2D)</cell><cell>(None, 32, 3, 5)</cell><cell>9248</cell></row><row><cell>max_pooling2d_3 (MaxPooling2D)</cell><cell>(None, 32, 2, 3)</cell><cell>0</cell></row><row><cell>flatten_1 (Flatten)</cell><cell>(None, 192)</cell><cell>0</cell></row><row><cell>dense_1 (Dense)</cell><cell>(None, 10)</cell><cell>1930</cell></row><row><cell>dense_2 (Dense)</cell><cell>(None, 1)</cell><cell>11</cell></row><row><cell>Total params: 19,981</cell><cell></cell><cell></cell></row><row><cell>Trainable params: 19,981</cell><cell></cell><cell></cell></row><row><cell>Non-trainable params: 0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 5 .</head><label>5</label><figDesc>6 Comparison of average accuracy among different models under 10-fold cross-validation in predicting glass-forming ability .</figDesc><table><row><cell>Model</cell><cell cols="2">Data representation</cell><cell cols="4">Input size Algorithms Average accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training</cell><cell>Testing</cell></row><row><cell>Ward's</cell><cell cols="2">Magpie general-purpose</cell><cell>145</cell><cell>Random</cell><cell>-</cell><cell>90%</cell></row><row><cell>work 58</cell><cell>descriptors</cell><cell></cell><cell></cell><cell>forest</cell><cell></cell><cell></cell></row><row><cell>SNN1</cell><cell cols="2">Manual features vector +</cell><cell>13+1</cell><cell>SNN</cell><cell>89.8%</cell><cell>89.9%</cell></row><row><cell></cell><cell cols="2">processing parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNN2</cell><cell cols="2">Composition vector +</cell><cell>73+1</cell><cell>SNN</cell><cell>93.2%</cell><cell>92.8%</cell></row><row><cell></cell><cell cols="2">processing parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNN3</cell><cell cols="2">Manual features vector +</cell><cell>86+1</cell><cell>SNN</cell><cell>93.9%</cell><cell>93.5%</cell></row><row><cell></cell><cell cols="2">composition vector +</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">processing parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNN4</cell><cell cols="2">Magpie general-purpose</cell><cell>145+1</cell><cell>SNN</cell><cell>90.1%</cell><cell>90.0%</cell></row><row><cell></cell><cell cols="2">descriptors + processing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN1</cell><cell cols="3">Atom table representation 11*11</cell><cell>CNN</cell><cell>96.4%</cell><cell>95.0%</cell></row><row><cell>CNN2</cell><cell>Randomized</cell><cell>periodic</cell><cell>9*18</cell><cell>CNN</cell><cell>96.7%</cell><cell>94.9%</cell></row><row><cell></cell><cell cols="2">table representation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN3</cell><cell>Periodic</cell><cell>table</cell><cell>9*18</cell><cell>CNN</cell><cell>96.4%</cell><cell>96.3%</cell></row><row><cell></cell><cell>representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 5 .</head><label>5</label><figDesc>7 Comparison of models' prediction accuracy on unseen alloy systems.</figDesc><table><row><cell>Alloy system</cell><cell cols="3">SNN3 SNN4 CNN1 CNN2 CNN3</cell></row><row><cell>Leave one system out test on 160 ternary</cell><cell>-</cell><cell>76.5% -</cell><cell>77.3% 83.8%</cell></row><row><cell>systems</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Leave one system out test on Al-Ni-Zr system</cell><cell cols="3">62.3% 63.5% 68.4% 66.4% 80.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>.7. CNN3 outperforms CNN2 and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 5 .</head><label>5</label><figDesc>7 also shows the LOSO cross-validation results for the Al-Ni-Zr system. Here, we used Al-Ni-Zr amorphous ribbon alloys results (5151 composition points in total, not the 296 Al-Ni-Zr entries in our dataset) in Figure 5.11(b) as ground truth to calculate prediction accuracy. The predictions of CNN3, CNN2, CNN1, SNN4 and SNN2 are shown in Figure 5.12. We can see CNN3 shows accuracy advantage over other CNNs and SNNs by at least 12% when no Al-Ni-Zr data is in training dataset. Overall, this exclusion test strongly verified the advantage of convolutional neural network plus periodic table</figDesc><table><row><cell>representation over other models using representation without domain knowledge and</cell></row><row><cell>using manual featuring engineering. CNN3 can be used to predict the glass-forming</cell></row><row><cell>ability in alloy systems that are entirely unassessed.</cell></row><row><cell>5.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>.3 Validation using alloy systems outside our dataset</head><label></label><figDesc></figDesc><table><row><cell>(1) Validation using Ir-Ni-Ta-(B) and Mg-Cu-Yb bulk metallic glasses</cell></row><row><cell>To further validate the prediction ability of the models, we collected some data from</cell></row><row><cell>newly reported alloy systems, e.g. high-temperature Ir-Ni-Ta-(B) 183 and Mg-Cu-Yb 184 bulk</cell></row><row><cell>metallic glasses, which are outside our dataset. Our dataset only has one entry about In-</cell></row><row><cell>Ni-Ta system, i.e., amorphous ribbon composition In1Ni59Ta40; our dataset does not have</cell></row><row><cell>any data about Mg-Cu-Yb ternary amorphous ribbon alloys or bulk metallic glasses.</cell></row><row><cell>These two alloys systems were developed using trials and errors guided by empirical</cell></row><row><cell>criteria.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 5 .</head><label>5</label><figDesc>8 shows the experimental results and the predictions of our models. The comparison agrees with that in the Al-Ni-Zr system: CNN3 using periodic table representation plus automatic feature engineering shows the highest prediction ability in unseen systems. SNN4 based on Magpie general purpose descriptors failed in predicting Ir-Ni-Ta-(B) and Mg-Cu-Yb bulk metallic glasses. On the other hand, SNN3 based on manual features derived from empirical criteria successfully predicted all Ir-Ni-Ta-(B) and Mg-Cu-Yb bulk metallic glasses. It shows the influence of feature engineering</figDesc><table><row><cell>on performance of a machine learning model. CNN2 using automatic feature</cell></row><row><cell>engineering but lack domain knowledge in data representation show lower accuracy</cell></row><row><cell>than CNN3 with periodic table knowledge in representation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 5</head><label>5</label><figDesc>Table 5.9 and Figure 5.13 show the convolutional neural network plus periodic table representation can infer sulfur's beneficial effect on glass-forming ability from its position in periodic table. Researchers have already known the similarity of Ni and Cu in affecting glass forming. Figure 5.13 shows the glass-forming ability curves of</figDesc><table><row><cell cols="5">.8 Comparison of experimental results with the predictions by CNN1, CNN2, CNN3, SNN3,</cell></row><row><cell cols="5">and SNN4 in Ir-Ni-Ta-(B), Mg-Cu-Yb alloys. The red indicates the wrong prediction, and the green</cell></row><row><cell cols="2">indicates the right prediction.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Experiment SNN3 SNN4 CNN1 CNN2 CNN3</cell></row><row><cell cols="2">Ir-Ni-Ta-(B) Ir35Ni20Ta40B5</cell><cell>BMG</cell><cell>BMG CR</cell><cell>BMG BMG BMG</cell></row><row><cell></cell><cell>Ir35Ni25Ta40</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG CR</cell><cell>BMG</cell></row><row><cell></cell><cell>Ir33Ni28Ta39</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG AMR BMG</cell></row><row><cell></cell><cell>Ir30Ni30Ta40</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG AMR BMG</cell></row><row><cell></cell><cell>Ir25Ni35Ta40</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG BMG BMG</cell></row><row><cell></cell><cell>Ir25Ni40Ta35</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG BMG BMG</cell></row><row><cell></cell><cell>Ir20Ni40Ta40</cell><cell>BMG</cell><cell cols="2">BMG AMR BMG BMG BMG</cell></row><row><cell>Mg-Cu-Yb</cell><cell>Mg55Cu36Yb9</cell><cell>BMG</cell><cell cols="2">BMG BMG AMR BMG BMG</cell></row><row><cell></cell><cell>Mg54.5Cu27.3Yb18.2</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg50Cu36.4Yb13.6</cell><cell>BMG</cell><cell cols="2">BMG BMG BMG BMG BMG</cell></row><row><cell></cell><cell>Mg50Cu30Yb20</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg45.5Cu36.4Yb18.2</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg44Cu40.5Yb15.5</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg43Cu30Yb27</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg40.8Cu36.4Yb22.9</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg36.4Cu27.3Yb36.4</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell></cell><cell>Mg33.4Cu33.3Yb33.3</cell><cell>BMG</cell><cell cols="2">BMG AMR AMR BMG BMG</cell></row><row><cell>Score</cell><cell></cell><cell></cell><cell cols="2">17/17 2/17 8/17 14/17 17/17</cell></row><row><cell cols="3">(2) Validation using sulfur-bearing alloys</cell><cell></cell></row><row><cell cols="5">Since sulfur is a direct neighbor of phosphorous in the periodic table and due to their</cell></row><row><cell cols="5">similar atomic diameter and similar electronegativity on Pauling's scale, sulfur may be</cell></row><row><cell cols="5">assumed to act like phosphorous in glass-forming compositions. Recently, Kuball et.al</cell></row><row><cell cols="5">reported on the formation of sulfur-bearing bulk glass-forming alloys 185,186 . Figure 5.2</cell></row><row><cell cols="5">shows that our dataset did not contain any S-containing amorphous ribbon alloys or bulk</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">9 Comparison of experimentally results with the predictions by CNN1, CNN2, CNN3,</cell></row><row><cell cols="5">SNN3, and SNN4 in sulfur-bearing alloys. The red indicates the wrong prediction, and the green</cell></row><row><cell cols="2">indicates the right prediction.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Experiment</cell><cell cols="2">SNN3 SNN4 CNN1 CNN2 CNN3</cell></row><row><cell>Sulfur-</cell><cell>Ti75Ni17S8</cell><cell>AMR</cell><cell>CR</cell><cell>AMR AMR AMR AMR</cell></row><row><cell>bearing</cell><cell>Ti70Zr5Ni12Cu5S8</cell><cell>BMG</cell><cell>CR</cell><cell>AMR AMR AMR AMR</cell></row><row><cell>alloys</cell><cell>Ti60Zr15Ni12Cu5S8</cell><cell>BMG</cell><cell>CR</cell><cell>AMR BMG AMR BMG</cell></row><row><cell></cell><cell>Ti50Zr25Ni12Cu5S8</cell><cell>BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>Ti40Zr35Ni12Cu5S8</cell><cell>BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>Pd31Ni42S27</cell><cell>BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>Pd37Ni37S26</cell><cell>BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>Zr56.5Ti13.3Ni13.6Cu9.6S7</cell><cell>BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>(Cu47Ti34Zr11Ni8)98.5S1.5</cell><cell>BMG</cell><cell>CR</cell><cell>AMR BMG BMG BMG</cell></row><row><cell></cell><cell cols="2">(Zr52.5Cu17.9Ni14.6Al10Ti5)98S2 BMG</cell><cell>CR</cell><cell>BMG BMG BMG BMG</cell></row><row><cell></cell><cell>(Ni62Nb38)97S3</cell><cell>BMG</cell><cell>CR</cell><cell>AMR BMG BMG BMG</cell></row><row><cell>Score</cell><cell></cell><cell></cell><cell cols="2">0/11 7/11 10/11 9/11 10/11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 5 .</head><label>5</label><figDesc>10 Comparison of experimental results with the predictions by CNN1, CNN2, CNN3, SNN3 and SNN4 in RE6Fe72B22. The red indicates the wrong prediction, and the green indicates the right prediction. Superscript GT denotes this composition is in our dataset; superscript * denotes this composition is not in our dataset, but some other compositions of this alloy system are in our</figDesc><table><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Experimental SNN3</cell><cell>SNN4</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell></row><row><cell cols="2">RE6Fe72B22 Sc6Fe72B22</cell><cell>BMG</cell><cell>BMG</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell cols="2">Y6Fe72B22 GT BMG</cell><cell>BMG</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell cols="2">La6Fe72B22 * AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell cols="2">Ce6Fe72B22 AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell>Pr6Fe72B22</cell><cell>AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell></row><row><cell></cell><cell cols="2">Nd6Fe72B22 * AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell></row><row><cell></cell><cell cols="2">Sm6Fe72B22 AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell><cell>AMR</cell></row><row><cell></cell><cell cols="2">Eu6Fe72B22 AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell><cell>AMR</cell></row><row><cell></cell><cell cols="2">Gd6Fe72B22 AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell></row><row><cell></cell><cell cols="2">Tb6Fe72B22 AMR</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell></row><row><cell></cell><cell cols="2">Dy6Fe72B22 BMG</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell cols="2">Ho6Fe72B22 * BMG</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell><cell>BMG</cell></row><row><cell></cell><cell>Er6Fe72B22</cell><cell>BMG</cell><cell>BMG</cell><cell>AMR</cell><cell>AMR</cell><cell>AMR</cell><cell>BMG</cell></row><row><cell>Score</cell><cell></cell><cell></cell><cell>5/13</cell><cell>8/13</cell><cell>7/13</cell><cell>8/13</cell><cell>10/13</cell></row><row><cell cols="8">Table 5.11 Comparison of experimentally measured glass-forming ability with the predictions by</cell></row><row><cell cols="8">CNN1, CNN2, CNN3, SNN3 and SNN4 in 18 binary outliers. The red indicates the wrong prediction,</cell></row><row><cell cols="8">and the green indicates the right prediction. Superscript GT denotes this composition is in our</cell></row><row><cell cols="8">dataset; superscript * denotes this composition is not in our dataset, but some other</cell></row><row><cell cols="4">compositions of this alloy system are in our dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Experimental SNN3</cell><cell>SNN4</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell></row><row><cell>Outliers</cell><cell>Ag88.5Y11.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>according</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>empirical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>criteria</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head></head><label></label><figDesc>It indicates the periodic table representation transfers the knowledge of periodic tableto the glass-forming ability knowledge, i.e., background knowledge was absorbed by the machine learning models. Figure5.19(b) illustrates the knowledge extracted by convolutional neural network from representation without periodic table background: no apparent periodic trends can be observed. The differences between Figure5.19(a)   </figDesc><table><row><cell>and Figure 5.19(b) indicate the knowledge extracted from randomized periodic table</cell></row><row><cell>representation is incomplete and fragmented due to our limited training data. The</cell></row><row><cell>difference explained why CNN3 shows better performance in predicting new data than</cell></row><row><cell>CNN1 and CNN2: CNN3 using periodic table representation can exploit periodic table'</cell></row><row><cell>table extracted by convolutional neural network with periodic table representation and</cell></row><row><cell>it shows apparent periodic trends: elements from 18 groups, lanthanide (group 19) and</cell></row><row><cell>actinides (group 20) are clustered in different regions (marked with different colors);18</cell></row><row><cell>groups distribute along a semicircle from group 1 to group 18 in sequence; elements</cell></row><row><cell>from lanthanide and actinides distribute in 2 semicircles with atomic number sequence;</cell></row><row><cell>elements in one group distributes from semicircle's inside to outside according to</cell></row><row><cell>ascending atomic number. Our dataset has limited data of more than half the elements</cell></row><row><cell>in the periodic table and does not have data about alkali metals (group 1), halogens</cell></row></table><note><p>(group 17), noble gases (group 18), but the trends of them are consistent and reasonable.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>of transfer learning to predicting phase formation in multi-principal element alloys and phase prototypes with small datasets</head><label></label><figDesc>Convolutional neural networks get domain knowledge (e.g. periodic table knowledge) embedded in 2-D representation through learning. Applying convolutional neural networks with enough data, the benefit of adding domain expertise (periodic table structure) to data representation is not obvious. However, domain expertise is vital if data is insufficient.Overfitting and low accuracy can be anticipated when applying convolutional neural network (that we used in glass-forming ability and OQMD prediction) to small datasets of multi-principal element alloys. In this scenario, we can resort to transfer learning to tackle the challenge of small datasets. In transfer learning, two correlated datasets, i.e., source dataset (usually a big/medium dataset) and target dataset (usually a small dataset) are assembled. The source dataset is used for the transferable feature extractor (the convolutional layers of a convolutional neural network). The feature extractor generates feature vectors for the target dataset. The feature vectors are then used for training shallow classifier/regressor, e.g., random forest, shallow neural network.</figDesc><table /><note><p>6 Applications</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head></head><label></label><figDesc>In 170 prototypes, 93 prototypes have entries dataset was used to train convolutional neural networks (in order to obtain the transferable and reusable feature extractors), feature vectors of the target dataset were generated by the reusable feature extractor; classification/regression was made based on the generated features by some shallow learning machines (random forest in this work) e.g. random forest (RF), support vector machine (SVM), shallow neural network (SNN). The features extractors can be directly used in many small new target datasets.Training new convolutional neural networks from scratch based on the new datasets is not necessary. Only new classifiers and regressors need to be built and trained. Our model without resorting to any manual features is capable of distinguishing BCC, FCC, HCP, amorphous, and multiple-phase mixture with 5-fold cross validation scores (accuracy, recall, precision and F1) over 94% after training and test. We should bear in mind that when labels' distribution is unbalanced like that of our multiprincipal element alloys data, achieving high recall, high precision and high accuracy at the same time is very difficult. We can see model transferred from CNN3 has the highest scores which indicates that periodic table representation is also beneficial for transfer learning. The proposed transfer learning model is an upgrade for conventional machine learning relying on manual feature engineering. It could serve as an effective guide for designing new multi-principal element alloys.</figDesc><table><row><cell>The two machine learning tasks, i.e. predicting glass-forming ability (GFA) and predicting</cell></row><row><cell>phases of multi-principal element alloys (MPEAs), have different output domain</cell></row><row><cell>(amorphous/crystalline binary classification in glass-forming ability prediction and 5</cell></row><row><cell>phases labels in multi-principal element alloys prediction) and highly correlated (or</cell></row><row><cell>overlapped) input domain from the point of transfer learning: Figure 5.2 and Figure 6.2(b)</cell></row><row><cell>atomic size difference, mixing enthalpy, mixing entropy, difference in Pauling</cell></row><row><cell>electronegativities, and valence electron concentration). So, we believe that the</cell></row><row><cell>automatic feature extractors of the well-trained convolutional neural networks, which</cell></row><row><cell>have outperformed known manual features in glass-forming ability prediction, will work</cell></row><row><cell>in multi-principal element alloys prediction too. In transfer learning from glass-forming</cell></row><row><cell>ability to multi-principal element alloys, compositions of multi-principal element alloys</cell></row><row><cell>4' is 'Cu, cF4, 225'. We were fed into the well-trained CNN1, CNN2, CNN3, and the intermediate results (high</cell></row><row><cell>selected a subset from the handbook: only prototypes that have more than 20 entries dimensional features yielded from convolutional layers) of theses convolutional neural</cell></row><row><cell>were selected; only room temperature stable phases were kept (high temperature networks were extracted. Then these features were used in new classifier (here we used</cell></row><row><cell>phases, high pressure phases, and low temperature phases were discarded); substances random forest for its good interpretability, and it need very little hyperparameters</cell></row><row><cell>of one element were not included. The final dataset has 17,762 inorganic substances and optimization) as input. Stratified data division strategy (to ensure training and testing</cell></row><row><cell>involves 170 phase prototypes. The elements distribution is shown in Figure 6.3. Most dataset have similar data distribution) and Scikit-learn package were used in training.</cell></row><row><cell>elements (92 out of 108) occur in our dataset, and the frequency of element occurrence</cell></row></table><note><p><p><p><p>is not so uniform like that in OQMD dataset. Figure</p>6</p>.4 shows the statistics of each phase prototype's entry number in the dataset.</p>show common elements in those alloys are similar; some amorphous alloys are also multi-principal element alloys; the descriptors developed in conventional machine learning for glass-forming ability and multi-principal element alloys can be shared (e.g. The well-trained feature extractors (the left part of the convolutional neural networks for predicting Ef and V) on OQMD dataset were reused in distinguishing 170 phase prototypes of inorganic materials. In transfer learning, chemistry composition of a metrics, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>Table 6 .</head><label>6</label><figDesc>2 The scores of transfer learning on multi-principal element alloys dataset under 5-fold cross-validation with three data representations</figDesc><table><row><cell>Data representation</cell><cell cols="4">Transfer from Accuracy Precision Recall</cell><cell>F1 Score</cell></row><row><cell cols="2">Periodic table representation CNN3</cell><cell>0.935</cell><cell>0.940</cell><cell>0.935</cell><cell>0.934</cell></row><row><cell>Randomized periodic table</cell><cell>CNN2</cell><cell>0.854</cell><cell>0.837</cell><cell>0.854</cell><cell>0.831</cell></row><row><cell>representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Atom table representation</cell><cell>CNN1</cell><cell>0.884</cell><cell>0.888</cell><cell>0.884</cell><cell>0.875</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head>Table 6 .</head><label>6</label><figDesc>4 The performance metrics of transfer learning models for predicting multi-principal</figDesc><table><row><cell cols="3">element alloy phases under 5-fold cross-validation</cell><cell></cell></row><row><cell>Fold 1</cell><cell cols="3">precision recall f1-score support</cell></row><row><cell>multi</cell><cell>0.91</cell><cell>0.95 0.93</cell><cell>44</cell></row><row><cell>fcc</cell><cell>0.80</cell><cell>0.80 0.80</cell><cell>5</cell></row><row><cell>amorphous</cell><cell>1.00</cell><cell>0.83 0.91</cell><cell>12</cell></row><row><cell>bcc</cell><cell>0.89</cell><cell>0.89 0.89</cell><cell>9</cell></row><row><cell>hcp</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell>3</cell></row><row><cell>macro avg</cell><cell>0.92</cell><cell>0.90 0.91</cell><cell>73</cell></row><row><cell cols="2">weighted avg 0.92</cell><cell>0.92 0.92</cell><cell>73</cell></row><row><cell>accuracy</cell><cell>0.92</cell><cell></cell><cell>73</cell></row><row><cell>Fold 2</cell><cell cols="3">precision recall f1-score support</cell></row><row><cell>multi</cell><cell>0.98</cell><cell>1.00 0.99</cell><cell>44</cell></row><row><cell>fcc</cell><cell>1.00</cell><cell>0.80 0.89</cell><cell>5</cell></row><row><cell>amorphous</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell>12</cell></row><row><cell>bcc</cell><cell>1.00</cell><cell>0.88 0.93</cell><cell>9</cell></row><row><cell>hcp</cell><cell>0.75</cell><cell>1.00 0.86</cell><cell>3</cell></row><row><cell>macro avg</cell><cell>0.95</cell><cell>0.93 0.93</cell><cell>72</cell></row><row><cell cols="2">weighted avg 0.98</cell><cell>0.97 0.97</cell><cell>72</cell></row><row><cell>accuracy</cell><cell>0.97</cell><cell></cell><cell>72</cell></row><row><cell>Fold 3</cell><cell cols="3">precision recall f1-score support</cell></row><row><cell>multi</cell><cell>0.89</cell><cell>0.95 0.92</cell><cell>43</cell></row><row><cell>fcc</cell><cell>1.00</cell><cell>0.60 0.75</cell><cell>5</cell></row><row><cell>amorphous</cell><cell>0.92</cell><cell>0.92 0.92</cell><cell>12</cell></row><row><cell>bcc</cell><cell>0.88</cell><cell>0.88 0.88</cell><cell>8</cell></row><row><cell>hcp</cell><cell>1.00</cell><cell>0.67 0.80</cell><cell>3</cell></row><row><cell>macro avg</cell><cell>0.94</cell><cell>0.80 0.85</cell><cell>71</cell></row><row><cell cols="2">weighted avg 0.91</cell><cell>0.90 0.90</cell><cell>71</cell></row><row><cell>accuracy</cell><cell>0.90</cell><cell></cell><cell>71</cell></row><row><cell>Fold 4</cell><cell cols="3">precision recall f1-score support</cell></row><row><cell>multi</cell><cell>0.91</cell><cell>1.00 0.96</cell><cell>43</cell></row><row><cell>fcc</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell>5</cell></row><row><cell>amorphous</cell><cell>1.00</cell><cell>0.83 0.91</cell><cell>12</cell></row><row><cell>bcc</cell><cell>1.00</cell><cell>0.75 0.86</cell><cell>8</cell></row><row><cell>hcp</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell>3</cell></row><row><cell>macro avg</cell><cell>0.98</cell><cell>0.92 0.94</cell><cell>71</cell></row><row><cell cols="2">weighted avg 0.95</cell><cell>0.94 0.94</cell><cell>71</cell></row><row><cell>accuracy</cell><cell>0.94</cell><cell></cell><cell>71</cell></row><row><cell>Fold 5</cell><cell cols="3">precision recall f1-score support</cell></row><row><cell>multi</cell><cell>0.95</cell><cell>0.95 0.95</cell><cell>43</cell></row><row><cell>fcc</cell><cell>1.00</cell><cell>0.75 0.86</cell><cell>4</cell></row><row><cell>amorphous</cell><cell>1.00</cell><cell>0.91 0.95</cell><cell>11</cell></row><row><cell>bcc</cell><cell>0.80</cell><cell>1.00 0.89</cell><cell>8</cell></row><row><cell>hcp</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell>2</cell></row><row><cell>macro avg</cell><cell>0.95</cell><cell>0.92 0.93</cell><cell>68</cell></row><row><cell cols="2">weighted avg 0.95</cell><cell>0.94 0.94</cell><cell>68</cell></row><row><cell>accuracy</cell><cell>0.94</cell><cell></cell><cell>68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head></head><label></label><figDesc>ConclusionsIn this thesis, three deep learning techniques (i.e., compact fully connected deep neural network plus pre-training, 2D periodic table data representation plus convolutional neural network, convolutional neural network plus transfer learning) were proposed to tackle the challenges in machine learning for materials research: small datasets and challenging manual feature engineering. Five case studies were carried out to validate their effectiveness: predicting solidification cracking susceptibility with a small dataset using compact fully connected deep neural network; predicting glass-forming ability and compound's forming energy and specific volume using periodic table representation plus convolutional neural network; predicting compound's phase prototype and phases of multi-principal element alloys through transfer learning with small datasets. The conclusions are shown as follows.Through machine learning, vast scattered experimental data hidden in literature can produce simple quantitative expression: material property as a function of chemistry composition and processing parameters.Though big dataset plus deep learning is an ideal solution, small dataset plus compact deep neural network and pre-training is a reasonable choice for small datasets are common in material research.Combination of deep neural network and tree-based models can bring us predictions of high accuracy and a certain insight of mechanisms.Mapping chemistry composition and processing parameters of materials to 2-D pseudoimages enables convolutional neural network process conventional structured data. This bypass the challenging manual feature engineering in machine learning. The feature extractors of convolutional neural networks that are well-trained on big datasets can be reused directly in new tasks to generate a rich set of general features. Transfer learning not only dramatically decreases training time of new models, but also improves accuracy and generalization of new models with small datasets.Convolutional neural networks get domain knowledge (e.g., periodic table knowledge) embedded in 2-D representation through learning. When dataset is large enough, the benefit of adding domain expertise to 2-D data representation is not apparent. However, domain expertise is vital for tasks of small datasets (limited data). Adding domain expertise to data representation improves models' generalization. Periodic table knowledge and is beneficial for the performance of convolutional neural networks and transfer learning models with limited data.</figDesc><table /><note><p>7.1</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>Firstly, I would like to thank <rs type="person">Prof Hongbiao Dong</rs> for supervising and supporting me throughout the PhD. I would also like to thank <rs type="person">Prof Huiyu Zhou</rs> for supervising and encouraging me on the way of machine learning. Special thanks go to <rs type="person">Prof Simon Gill</rs> and <rs type="person">Charlotte Ratcliffe</rs> for helping me in the past four years.</p><p>I would also like to thank everyone, especially my friends in <rs type="institution">IMPACT CDT</rs>, <rs type="person">Prof Hongbiao Dong</rs>'s group, and <rs type="person">Prof Huiyu Zhou</rs>'s group, who helped me in one way or another during the PhD.</p><p>Last but not least, I wish to acknowledge <rs type="funder">EPSRC CDT</rs> (Grant No: <rs type="grantNumber">EP/L016206/1</rs>) in <rs type="funder">Innovative Metal Processing (IMPACT)</rs> for providing a PhD studentship for this study.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CubrfdG">
					<idno type="grant-number">EP/L016206/1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and code availability</head><p>The datasets and codes used to generate the results in this thesis are available at https://github.com/sf254/SC, https://github.com/sf254/glass-froming-ability-prediction, and https://github.com/sf254/phase-prototypes-prediction.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>AI: artificial intelligence machine's predictions, (d) random forest's predictions, and (e) decision tree's predictions.   Besides the intuitive impression of feature importance, tree-based machine learning models can give the value of feature importance. Figure <ref type="figure">4</ref>.13 is the feature importance for solidification cracking susceptibility derived from the optimal random forest model.</p><p>Major alloy elements Ni and Cr, minor alloy elements Mn and Si, impurity elements C, N, P, S, and the strain applied on the specimen play the most important roles in determining the solidification cracking susceptibility of stainless steels. The knowledge derived can be used to guide the next step predictions. A serial of predictions about solidification cracking susceptibility dependence on compositions and processing parameters in the following part will give us more details and a deeper understanding of this complex problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.3">Visualization of prediction</head><p>(1) Solidification cracking susceptibilities of 304 and 310S stainless steels decreases from top to bottom within a group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials properties originated from electrons' behaviors, and periodic table embodies</head><p>the electron distribution of outer shells. Periodic table has abundant physical and chemical knowledge (see Figure 5.15). Atomic radius, Pauli electronegativity, valence electrons density, and other physical chemistry properties display periodic variations in periodic table. When developing new amorphous alloys, periodic table is often used as a map; similar atom substitution and column substitution are common strategies for improving glass-forming ability. The spatial information or elements' relative position information is difficult to be fully described by manual features engineering (a 1-D vector). The solution is keeping the periodic table structure in representation. So, adding periodic table structure into data representations affords models the ability to infer useful information from the periodic table when direct data is insufficient.</p><p>To improve model interpretability and prove the benefit of using representations with periodic table structure, we illustrated the information convolutional neural networks extract from different representations. Visualizing the high dimensional features extracted by convolutional layers, i.e., the intermediate results of convolutional neural networks, is a good way to explore the extracted features.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Testing results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Testing accuracy on multi-principal element alloys dataset</head><p>Table <ref type="table">6</ref>.2 shows the scores of our transfer learning models on multi-principal element alloys dataset under 5-fold cross-validation. Figure <ref type="figure">6</ref>.5, Table <ref type="table">6</ref>.3, and Table <ref type="table">6</ref>.4 show the ROC (receiver operating characteristic) curves, confusion matrix, and performance</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting solidification cracking susceptibility of stainless steels using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1088/1757-899X/861/1/012073</idno>
		<ptr target="https://doi.org/10.1088/1757-899X/861/1/012073" />
	</analytic>
	<monogr>
		<title level="j">IOP Conference Series: Materials Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">861</biblScope>
			<biblScope unit="page">12073</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using deep neural network with small dataset to predict material defects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.matdes.2018.11.060</idno>
		<ptr target="https://doi.org/10.1016/j.matdes.2018.11.060" />
	</analytic>
	<monogr>
		<title level="j">Materials and Design</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="300" to="310" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chapter 17 -Applications of artificial intelligence in material design and processing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and future social development (in Chinese), Scientific and technical documentation press</title>
		<editor>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</editor>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="315" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vaporization of Ni, Al and Cr in Ni-Base alloys and its influence on surface defect formation during manufacturing of single-crystal components</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kobertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>D'souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11661-019-05498-1</idno>
		<ptr target="https://doi.org/10.1007/s11661-019-05498-1" />
	</analytic>
	<monogr>
		<title level="j">Metallurgical and Materials Transactions A</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revealing internal flow behavior in arc welding and additive manufacturing of metals</title>
		<author>
			<persName><forename type="first">L</forename><surname>Aucott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mirihanage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kidess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Connolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Mathiesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Atkinson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-07900-9</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-07900-9Contents" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Literature review on machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Definition of machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Machine learning workflow</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Basic conceptions of machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3.1 Data, classification &amp; regression</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training &amp; testing datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Overfit</surname></persName>
		</author>
		<author>
			<persName><surname>Underfit</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3.4 Loss function, backpropagation algorithm &amp; regularization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Typical machine learning models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">4.1 Fully connected shallow &amp; deep neural network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional neural network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Support vector machine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ensemble and bagging</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Challenges in machine learning for materials research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">1 Data-wise challenges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Descriptor-wise challenges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">3 Task-wise challenges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Literature review on solidification cracking &amp; phase formation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Solidification cracking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">1.1 Hot cracking and solidification cracking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The complexity of solidification cracking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mechanical factors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Solidification cracking of stainless steel</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Solidification cracking susceptibility test</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Machine learning for solidification cracking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Phase formation prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">2.1 Amorphous phase and glass-forming ability</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-principal element alloys</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">3 Phase prototypes of inorganic substances</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">4 Application of deep neural network and pre-training to predicting solidification cracking susceptibility with small dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">2 Data pre-process &amp; dataset division</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Data pre-process</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Training &amp; testing datasets division</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Machine learning models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">3.1 Fully connected shallow &amp; deep neural networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Support vector machine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tree-based models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training and testing results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">6 Validation of solidification cracking susceptibility prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">7 Visualization of prediction and interpretability</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">Neural network equations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Visualization of prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Application of convolutional neural network &amp; periodic table representation to predicting glass-forming ability &amp; compound properties</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">1.1 Glass-forming ability dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">2 Open quantum materials database dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">D representation for data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Manual features engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Magpie descriptors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">3 VGG-like convolutional neural network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">5 Results of glass-forming ability prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Training and testing results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">5.2 Leave-one-system-out cross-validation results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">5.3 Validation using alloy systems outside our dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">6 Results of compound properties prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">7 The benefits of periodic table representation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">6 Applications of transfer learning to predicting phase formation in multi-principal element alloys and phase prototypes with small datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">1.1 Multi-principal element alloys dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">1.2 Phase prototype dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Details of transfer learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">3.1 Testing accuracy on multi-principal element alloys dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">3.2 Testing accuracy on phase prototypes dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">4 Visualization of the automatically-extracted features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">4.1 Features analysis for multi-principal element alloys</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">4.2 Features analysis for phase prototypes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Data and code availability</title>
	</analytic>
	<monogr>
		<title level="j">Zn14Gd2Co3</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="140" to="143" />
			<date>cI56</date>
		</imprint>
	</monogr>
	<note>NaBe4SbO7</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">U2PbSe5,mP32,14 NdBi2ClO4</title>
	</analytic>
	<monogr>
		<title level="j">tP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
	<note>W6Fe7. 7,123 U2Pt2Sn,tP20</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rb0.81W3O9</title>
	</analytic>
	<monogr>
		<title level="j">hP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="139" to="140" />
			<date>193</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">CeNiC2</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="201" to="238" />
			<date>Co0.5Ga0.54Ga,tI80,140 39 YbFe2Al10. 63 HgMo6Cl14</date>
			<publisher>Co9</publisher>
		</imprint>
	</monogr>
	<note>Ca</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bi0.33)3F15</title>
	</analytic>
	<monogr>
		<title level="j">Zr(Zr0</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">167 Sb2S3</biblScope>
			<biblScope unit="page" from="62" to="100" />
		</imprint>
	</monogr>
	<note>hR</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Tl0.39V3S4</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="55" to="87" />
			<date type="published" when="1236">0.83V5S8,mS28,12 36. cI38,204 KTb3F12. tI32. aP17,2 Ni3P. Co19P12</date>
		</imprint>
	</monogr>
	<note>Cr2Cl9. 148 Lu5Ni2In4,oP22</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Zr6Ni20P13</title>
	</analytic>
	<monogr>
		<title level="j">La3Ni2Ga2</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="194" to="224" />
			<date>cI40,220 30</date>
		</imprint>
	</monogr>
	<note>Hf9Mo4B</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Re5C15</title>
	</analytic>
	<monogr>
		<title level="j">KGdNb6Cl18</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="140" to="168" />
			<date type="published" when="7129">191 29. 58 U3Ni4Si4,oI22,71 29</date>
		</imprint>
	</monogr>
	<note>CrB7. Ba2Cu4YO8. 0.67Sr0.3...,hP22,186 29 ScU3S6. Cu2Sb,tP6,129 Tl4PbTe3</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">4O8</title>
	</analytic>
	<monogr>
		<title level="j">BaTiSi3O9</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="74" to="76" />
			<date type="published" when="5826">221. 12 Pr3WCl3O6. Fe2Si4.9,hP20,194 FeAs2. 6,58 26 Lu2Co3Si5,mS40,15 La2Sb,tI12. tI6,139 24 CaNb2Bi2O9</date>
		</imprint>
	</monogr>
	<note>KAsF6. 24,62 BiI3,hR24</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">OQMD dataset) and the smaller target dataset (phase prototypes dataset)</title>
	</analytic>
	<monogr>
		<title level="m">2 Details of transfer learning Two types of datasets were assembled in transfer learning: the big source dataset</title>
		<imprint/>
	</monogr>
	<note>e.g.. source Reference</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Introduction to Machine Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Machine learning phases of matter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Phys</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="431" to="434" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Using deep learning to model the hierarchical structure and function of a cell</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="290" to="298" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep learning massively accelerates super-resolution localization microscopy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aristov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lelek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="460" to="468" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Machine learning at the energy and intensity frontiers of particle physics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">560</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning phase transitions by confusion</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P L</forename><surname>Van Nieuwenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Phys</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="435" to="439" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Text mining facilitates materials discovery</title>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">571</biblScope>
			<biblScope unit="page" from="42" to="43" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Machine learning for molecular and materials science</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page" from="547" to="555" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Computationally guided discovery of thermoelectric materials</title>
		<author>
			<persName><forename type="first">G</forename><surname>Prashun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vladan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Eric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The Fourth Paradigm: Data-Intensive Scientific Discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tansley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth Paradigm: Data-Intensive Scientific Discovery</title>
		<imprint>
			<publisher>Microsoft Research</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Transfer learning. in Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IGI Global</publisher>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Neural Networks in Materials Science</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H</forename><surname>Bhadeshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Neural network design</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Demuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Beale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Martin Hagan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A Practical Bayesian Framework for Backpropagation Networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">An Introduction to Statistical Learning: with Applications in R</title>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kopec</surname></persName>
		</author>
		<title level="m">Artificial Intelligence in the 21st Century</title>
		<imprint>
			<publisher>Mercury Learning &amp; Information</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Predicting glass transition temperatures using neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P L F</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Zanotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Effects of carbon concentration and cooling rate on continuous cooling transformations predicted by artificial neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Der Wolk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Zwaag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1038" to="1046" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Modelling of transition from upper to lower bainite in multi-component system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Lembke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H</forename><surname>Bhadeshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mater. Sci. Technol. (United Kingdom)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Revisiting Stacking Fault Energy of Steels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metall. Mater. Trans. A Phys. Metall. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="748" to="768" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Application of neural networks for prediction of critical values of temperatures and time of the supercooled austenite transformations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>DobrzaÅ„ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trzaska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mater. Process. Technol</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">156</biblScope>
			<biblScope unit="page" from="1950" to="1955" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Modelling of CCT diagrams for engineering and constructional steels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Trzaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>DobrzaÅ„ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mater. Process. Technol</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="issue">193</biblScope>
			<biblScope unit="page" from="504" to="510" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Comparison of artificial neural networks with Gaussian processes to model the yield strength of nickelbase superalloys</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tancret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H</forename><surname>Bhadeshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1020" to="1026" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Application of constitutive and artificial neural network models to predict the hot strength of steels</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="991" to="998" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Application of neural networks to mechanical property determination of Ni-base superalloys</title>
		<author>
			<persName><forename type="first">J</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1006" to="1014" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Estimation of hot torsion stress strain curves in iron alloys using a neural network analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H</forename><surname>Bhadeshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="999" to="1005" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Statistical modelling of mechanical tensile properties of steels by using Neural Networks and Multivariate Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dumortier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lehert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="980" to="985" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Petri neural network model for the effect of controlled thermomechanical process parameters on the mechanical properties of HSLA steels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="986" to="990" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Use of neural networks for alloy design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1015" to="1019" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Design of a nickel-base superalloy using a neural network</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Conduit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Conduit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mater. Des</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="358" to="365" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Application of neural networks for designing the chemical composition of steel with the assumed hardness after cooling from the austenitising temperature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Trzaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>DobrzaÅ„ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mater. Process. Technol</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">165</biblScope>
			<biblScope unit="page" from="1637" to="1643" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Artificial intelligence/machine learning in manufacturing and inspection: A GE perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Aggour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MRS Bulletin</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Neural network approach to the prediction of submerged arc weld metal chemistry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Perez-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Warters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thewlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1096" to="1105" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Mold level control in continuous caster by neural network model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Omura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1053" to="1060" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Application of neural networks for quality evaluation of resistance spot welds</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dilthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dickersbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1061" to="1066" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Neural networks applied to welding: Two examples</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vitek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1088" to="1095" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Application Artificial Neural Network to Discrimination of Defect Type in Automatic Radiographic Testing of Welds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1081" to="1087" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Measurement of molten pool shape and penetration control applying Neural Network in TIG welding of thin steel plates</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Suga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Usui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1075" to="1080" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Forecasting heat levels in blast furnaces using a neural network model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1047" to="1052" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Using Al-methods for parameter scheduling, quality control and weld geometry determination in GMA-welding</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dilthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Preface to the Special Issue on &apos;Application of Neural Network Analysis in Materials Science</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H</forename><surname>Bhadeshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIJ Int</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">965</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Performance of neural networks in materials science</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K D H D H</forename><surname>Bhadeshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dimitriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Forsik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mater. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="504" to="510" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Modelling the correlation between processing parameters and properties in titanium alloys using artificial neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="375" to="394" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Deep Learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Material structureproperty linkages using three-dimensional convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cecen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Yabansu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kalidindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Microstructure recognition using convolutional neural networks for prediction of ionic conductivity in ceramics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Masuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Asahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A machine learning approach for engineering bulk metallic glass alloys</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A general-purpose machine learning framework for predicting properties of inorganic materials</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16028</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Machine learning modeling of superconducting critical temperature</title>
		<author>
			<persName><forename type="first">V</forename><surname>Stanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Predicting solidification cracking susceptibility of stainless steels using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IOP Conf. Ser. Mater. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">861</biblScope>
			<biblScope unit="page">12073</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Overcoming data scarcity with transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Hutchinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05099</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Detection and Segmentation of Manufacturing Defects with Convolutional Neural Networks and Transfer Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Smart Sustain. Manuf. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20180033</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">The Materials Genome Initiative, the interplay of experiment, theory and computation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>De Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ozolins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Ramirez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Solid State and Materials Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">New frontiers for the materials genome initiative</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>De Pablo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Computational Materials</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">The materials genome initiative: One year on</title>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MRS Bulletin</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="715" to="716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">AiiDA: automated interactive infrastructure and database for computational science</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cepellotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Marzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kozinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="218" to="230" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">The Open Quantum Materials Database (OQMD): Assessing the accuracy of DFT formation energies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15010</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">The search for high entropy alloys: A high-throughput ab-initio approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Toher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Vecchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Curtarolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="364" to="383" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">AFLOW: An automatic framework for high-throughput materials discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Curtarolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="218" to="226" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Commentary: The materials project: A materials genome approach to accelerating materials innovation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APL Mater</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11002</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A strategy to apply machine learning to small datasets in materials science</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Using deep neural network with small dataset to predict material defects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mater. Des</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="300" to="310" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Is domain knowledge necessary for machine learning materials properties?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kauwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sparks</surname></persName>
		</author>
		<idno type="DOI">10.26434/chemrxiv.11879193.v1</idno>
	</analytic>
	<monogr>
		<title level="j">ChemRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Machine learning in materials discovery: Confirmed predictions and their underlying approaches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Mater. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="49" to="69" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Can machine learning identify the next high-temperature superconductor? Examining extrapolation performance for materials discovery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Des. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="819" to="825" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Can machine learning find extraordinary materials?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kauwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sparks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page">109498</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lookman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Computational Materials</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Adaptive Strategies for Materials Design using Uncertainties</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lookman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">High-Dimensional Materials and Process Optimization Using Data-Driven Experimental Design with Well-Calibrated Uncertainty Estimates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integr. Mater. Manuf. Innov</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">A quantitative uncertainty metric controls error in neural network-driven chemical discovery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Janet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="7913" to="7922" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">A Gaussian Process Modeling Approach for Fast Robust Design With Uncertain Inputs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1115/gt2018-83</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ASME International</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Critical Temperature Prediction for a Superconductor: A Variational Bayesian Neural Network Approach</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Appl. Supercond</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Embedding domain knowledge for machine learning of complex material systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Washburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MRS Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="806" to="820" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Neural-Backed Decision Trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Recent advances and applications of machine learning in solid-state materials science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Botti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Complete Casting Handbook: Metal Casting Processes, Metallurgy, Techniques and Design</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Hot cracking tests-The route to International Standardization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C M</forename><surname>Farrar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot cracking phenomena in welds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="291" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">On the origin of weld solidification cracking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot cracking phenomena in welds 3-18</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Contribution to HAZ Liquation Cracking of Austenitic Stainless Steels</title>
		<author>
			<persName><forename type="first">P</forename><surname>BernasovskÃ½</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot cracking phenomena in welds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="84" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">New insight into the mechanism of ductility-dip cracking in Ni-base weld metals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lippold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Cracking Phenomena in Welds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Mechanical properties in the semi-solid state and hot tearing of aluminium alloys</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G G</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Suyitno &amp; Katgerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Materials Science</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="629" to="711" />
			<date type="published" when="2004">2004</date>
			<publisher>Pergamon</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A simple test for assessing solidification cracking susceptibility and checking validity of susceptibility prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Soysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="181" to="197" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">A criterion for cracking during solidification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="366" to="374" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">A new hot-tearing criterion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rappaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><forename type="middle">M</forename><surname>Drezet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gremaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metall. Mater. Trans. A</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="449" to="455" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Hot tearing of aluminum alloys a critical literature review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Apelian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Met</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="23" to="40" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Modelling of hot cracking in welding with a cellular automaton combined with an intergranular fluid flow model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bordreuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="442" to="450" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Modelling hot cracking in 6061 aluminium alloy weld metal with microstructure based criterion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Niel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bordreuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deschaux-Beaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Technol. Weld. Join</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="154" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Modeling of hot tearing and other defects in casting processes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASM Handb</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Recent developments in weldability testing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lippold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot cracking phenomena in welds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rappaz</surname></persName>
		</author>
		<title level="m">Solidification: 2nd Edition -Revised &amp; Expanded</title>
		<imprint>
			<publisher>EPFL Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Last-stage solidification of alloys: theoretical model of dendrite-arm and grain coalescence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rappaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Boettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metall. Mater. Trans. A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">In situ neutron diffraction during casting: determination of rigidity point in grain refined al-Cu alloys</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Drezet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mireux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Szaraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pirling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Materials</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2014">2014</date>
			<pubPlace>Basel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Determination of coherency and rigidity temperatures in Al-Cu alloys using in situ neutron diffraction during casting</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Drezet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mireux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Szaraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pirling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOM</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Hot tearing in polycrystalline Ni-based IN738LC superalloy: Influence of Zr content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Drezet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mater. Process. Technol</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Testing for susceptibility to hot cracking on Gleeble TM physical simulator</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Mandziej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot cracking phenomena in welds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2014</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">108</biblScope>
		</imprint>
	</monogr>
	<note>Hot cracking tests-an overview of present technologies and applications</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Hot ductility and hot cracking behavior of modified 316 stainless steels designed for high</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Batten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. Res. Counc. Bull</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="1993">2006. 1993</date>
			<publisher>JOURNAL-NEW YORK</publisher>
		</imprint>
	</monogr>
	<note>Weldability and hot ductility behavior of nuclear grade austenitic stainless steels. temperature service. Weld</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Solidification Crack Susceptibility in Weld Metals of Fully Austenitic Stainless Steels (Report VI): Effect of La or REM Addition on Solidification Crack Resistance (MATERIALS METALLURGY AND WELDABILITY)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Solidification Crack Susceptibility in Weld Metals of Fully Austenitic Stainless Steels (Report IX): Effect of Titanium on Solidification Crack Resistance (Materials, Metallurgy &amp; Weldability)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Hot Cracking Susceptibility of Austenitic Stainless Steels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsunetomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. J</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Solidification Crack Susceptibility in Weld Metals of Fully Austenitic Stainless Steels (Report VIII): Effect of Nitrogen on Cracking in SUS 304 Weld Metal (Materials, Metallurgy &amp; Weldability)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Solidification crack susceptibility in weld metals of fully austenitic stainless steels (report VII) : effect of Mn and N on solidification crack resistence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Solidification crack susceptibility in weld metals of fully austenitic stainless steels (report V): solidification crack susceptibility and amount of phosphide and sulphide in sus 310s weld metals</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Solidification crack susceptibility in weld metals of fully austenitic stainless steels (Report III): effect of strain rate on cracking threshold in weld metal during solidification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">117</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Solidification crack susceptibility in weld metals of fully austenitic stainless steels (Report II): Effect of ferrite, P, S, C, Si and Mn on ductility properties of solidification brittleness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Solidification crack susceptibility in weld metals of fully austenitic stainless steels (Report I): fundamental investigation on solidification behavior of fully austenitic and duplex microstructures and effect of ferrite on microsegregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Solidification Crack Susceptibility in Weld Metals of Fully Austenitic Stainless Steels (Report IV): Effect of Decreasing P and S on Solidification Crack Susceptibility of SUS 310S Austenitic Stainless Steel Weld Metals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Varestraint test for solidification crack susceptibility in weld metal of austenitic stainless steels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saruwatari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. JWRI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Weldability and Solidification Phenomena of Cast Stainless Steels</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. J</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Weldability of ferritic stainless steels</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A H</forename><surname>Dh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. J</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">An investigation of weld cracking in Alloy 800</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lippold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. J</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Solidification and solidification cracking in nitrogenstrengthened austenitic stainless steels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metall. Trans. A</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Weldability of newly developed austenitic alloys for cryogenic service: Part II-High-nitrogen stainless steel weld metal</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koseki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. J</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">126</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Characterization of weld solidification cracking in a duplex stainless steel</title>
		<author>
			<persName><forename type="first">I</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Baeslack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lippold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metallography</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Investigation of joining techniques for advanced austenitic alloys</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P S</forename><surname>Gill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Tennessee Univ</publisher>
			<biblScope unit="page">128</biblScope>
			<pubPlace>TN (United States; Knoxville, TN â€¦</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Evaluation of Backfilled Solidification Cracks in Austenitic Stainless Welds in Relationship to Evaluating Hot Cracking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y P</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. JOURNAL-NEW YORK</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Effect of welding parameters on hot cracking susceptibility of alloy 800</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. JAPAN Weld. Soc</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Cracking susceptibility of stainless steel subjected to plasma disruption</title>
		<author>
			<persName><forename type="first">H</forename><surname>Madarame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sukegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fusion Eng. Des</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">The solidification and welding metallurgy of galling-resistant stainless steels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Robino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. JOURNAL-NEW YORK</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Evaluation of hot cracking in nitrogen-bearing and fully austenitic stainless steel weldments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P S</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. Journal-Including Weld. Res. Suppl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">133</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Weld Microstructure Development and Properties of Precipitation-Strengthened Martensitic Stainless Steals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Garrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. JOURNAL-NEW YORK</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">The effects of phosphorus and sulfur on susceptibility to weld hot cracking in austenitic stainless steels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Messler</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. JOURNAL-NEW YORK</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Criteria for hot cracking evaluation in austenitic stainless steel welds using longitudinal varestraint and transvarestraint tests</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P S</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Technol. Weld. Join</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">136</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Weld solidification and cracking behavior of free-machining stainless steel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Robino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Headley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. JOURNAL-NEW YORK</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Effect of nitrogen addition on microstructure and fusion zone cracking in type 316L stainless steel weld metals</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Messler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. Res. Counc. Bull</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note>Mater. Sci. Eng. A</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Effect Of Phosphorous and Silicon On Hot Cracking Susceptibility Of 14Cr-15Ni-2.3 Mo Ti-Modified Fully-Austenitic Stainless Steel</title>
		<author>
			<persName><forename type="first">G</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhaduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weld. World</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Model for solidification cracking in low alloy steel weld metals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ichikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bhadeshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Technol. Weld. Join</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Recent development and application products of bulk glassy alloys</title>
		<author>
			<persName><forename type="first">A</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">142</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Fatigue and fracture behavior of bulk metallic glasses and their composites</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">The fracture of bulk metallic glasses</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">144</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Modern soft magnets: Amorphous and nanocrystalline materials</title>
		<author>
			<persName><forename type="first">G</forename><surname>Herzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A review of catalytic performance of metallic glasses in wastewater treatment: Recent progress and prospects</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Bulk metallic glasses with functional physical properties</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Mater</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Structural heterogeneities and mechanical behavior of amorphous alloys</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Non-crystalline Structure in Solidified Gold-Silicon Alloys</title>
		<author>
			<persName><forename type="first">W</forename><surname>Klement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Willens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duwez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page">149</biblScope>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">A new glass-forming ability criterion for bulk metallic glasses</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Classification of bulk metallic glasses by atomic size difference, heat of mixing and period of constituent elements and its application to characterization of the main alloying element</title>
		<author>
			<persName><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mater. Trans</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">151</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Role of different factors in the glass-forming ability of binary alloys</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Louzguine-Luzgin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">152</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">The influence of efficient atomic packing on the constitution of metallic glasses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Senkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Mag</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">A predictive structural model for bulk metallic glasses</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Laws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Confusion by design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Greer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Spectral descriptors for bulk metallic glasses based on the thermodynamics of competing crystalline phases</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">156</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Probabilistic Assessment of Glass Forming Ability Rules for Metallic Glasses Aided by Automated Analysis of Phase Diagrams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">157</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Electronegativity of the constituent rare-earth metals as a factor stabilizing the supercooled liquid region in Al-based metallic glasses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Louzguine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Lett</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Machine learning approach for prediction and understanding of glass-forming ability</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Chem. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawazoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-P</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masumoto</surname></persName>
		</author>
		<idno type="DOI">10.1007/b58222160</idno>
	</analytic>
	<monogr>
		<title level="m">Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">High-Entropy Alloys</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-27013-5162</idno>
	</analytic>
	<monogr>
		<title level="m">High-entropy alloys: Fundamentals and applications</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2019. 2016</date>
		</imprint>
	</monogr>
	<note>High-Entropy Alloys: Fundamentals and Applications</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">High-entropy alloys</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">163</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Low-density, refractory multi-principal element alloys of the Cr-Nb-Ti-V-Zr system: Microstructure and phase analysis</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Senkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Senkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">164</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">A critical review of high entropy alloys and related concepts</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Senkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">165</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Accelerated exploration of multi-principal element alloys for structural applications</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Senkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calphad</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">166</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Accelerated exploration of multi-principal element alloys with solid solution phases</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Senkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Machine learning for phase selection in multi-principal element alloys</title>
		<author>
			<persName><forename type="first">N</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page">168</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Machine learning guided appraisal and exploration of phase design for high entropy alloys</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Phase prediction in high entropy alloys with a rational selection of materials descriptors and machine learning models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actamat.2019.11.067170</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Machine learning assisted design of high entropy alloys with desired property</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Machine-learning model for predicting phase formations of high-entropy alloys</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">172</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Machine Learning and Energy Minimization Approaches for Crystal Structure Predictions: A Review and New Horizons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Graser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kauwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sparks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int. Conf. Learn. Represent. ICLR 2016 -Conf. Track Proc</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">174</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Robust welding technologies for ferrous alloys</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhaduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="DOI">10.1533/9781845695453.1.34175</idno>
	</analytic>
	<monogr>
		<title level="j">Weld Cracking in Ferrous Alloys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">95</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Elsevier Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Machine learning material properties from the periodic table using convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Atom table convolutional neural networks for an accurate prediction of compounds properties</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">177</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">An assessment of binary metallic glasses: correlations between structure, glass forming ability and stability</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Louzguine-Luzgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Louzguina-Luzgina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Mater. Rev</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Materials design and discovery with high-throughput density functional theory: The open quantum materials database (OQMD)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Crystallography Open Database -An open-access collection of crystal structures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Crystallogr</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Efficient local packing in metallic glasses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miracle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Non. Cryst. Solids</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Phase stability in high entropy alloys: formation of solidsolution phase or amorphous phase</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Nat. Sci. Mater. Int</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">182</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">High-temperature bulk metallic glasses developed by combinatorial methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, {ICLR} 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015. 2019</date>
			<biblScope unit="volume">569</biblScope>
			<biblScope unit="page">184</biblScope>
		</imprint>
	</monogr>
	<note>Very deep convolutional networks for large-scale image recognition</note>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Exceptionally broad bulk metallic glass formation in the Mg-Cu-Yb system</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Shamlaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Laws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>LÃ¶ffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Sulfur-bearing metallic glasses: A new family of bulk glass-forming alloys</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bochtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scr. Mater</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">On the bulk glass formation in the ternary Pd-Ni-S system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mater</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">187</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Soft magnetic ternary iron-boron-based bulk metallic glasses</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Lett</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Thermodynamics of concentrated solid solution alloys</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Solid State Mater. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<title level="m" type="main">Handbook of Inorganic Substances</title>
		<author>
			<persName><forename type="first">P</forename><surname>Villars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cenzual</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Walter De Gruyter Incorporated</publisher>
			<biblScope unit="page">190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">The PAULING FILE Project and Materials Platform for Data Science: From Big Data Toward Materials Genome</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Villars</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44677-</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Materials Modeling</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1837" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
